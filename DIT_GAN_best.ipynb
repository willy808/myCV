{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771c74f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import math\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import torch, os\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from torch.utils.data import Dataset\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8855145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "#dataset for cbct dicom\n",
    "def load_dcm(image_path):\n",
    "#     image_data=[]\n",
    "#     for i in range(len(image_path)):\n",
    "    ds=pydicom.dcmread(image_path)\n",
    "    ww=ds.WindowWidth\n",
    "    wc=ds.WindowCenter\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "\n",
    "    ymin = 0\n",
    "    ymax = 255\n",
    "\n",
    "    pixel_array = ds.pixel_array * slope + intercept\n",
    "\n",
    "    # linear exact array\n",
    "    linear_exact_array = np.zeros(pixel_array.shape)\n",
    "\n",
    "    linear_exact_array_less_idx = pixel_array <= (wc - ww/2)\n",
    "    linear_exact_array_large_idx = pixel_array > (wc + ww/2)\n",
    "    linear_exact_array = ((pixel_array - wc)/ww + 0.5) * (ymax - ymin) + ymin\n",
    "    linear_exact_array[linear_exact_array_less_idx] = ymin\n",
    "    linear_exact_array[linear_exact_array_large_idx] = ymax\n",
    "    linear_exact_array = linear_exact_array.astype(np.uint8)        \n",
    "    #add channel\n",
    "\n",
    "    ct=np.expand_dims(linear_exact_array,axis=0)\n",
    "    ct = np.transpose(ct, (2, 1, 0))\n",
    "    ct = np.clip(ct, -200, 2000)\n",
    "    # ct=normalize(ct)        \n",
    "    #because ndarray does not support uint16 , must change to float\n",
    "    ct=ct.astype(float)\n",
    "\n",
    "    return ct #image_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load data from .npy file\n",
    "data = np.load('./result.npy')\n",
    "\n",
    "# Create an empty numpy array to store the loaded data\n",
    "data_load = np.empty((len(data), 2), dtype=object)\n",
    "\n",
    "# Load each pair of data and store it in the numpy array\n",
    "for i, pair in enumerate(data):\n",
    "    data_load[i, 0] = load_dcm(pair[0])\n",
    "    data_load[i, 1] = load_dcm(pair[1])\n",
    "    \n",
    "# print(len(data_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f6be6f3-543f-4f49-9e93-2d2899e30d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(load_dcm(pair[0].shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ff1cf8a-50b8-4fca-a605-4f76b4ab6279",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms.functional as FT\n",
    "import random\n",
    "\n",
    "def apply_augmentation(ndarray):\n",
    "    # convert ndarray to PIL Image\n",
    "    ndarray = ndarray.astype('uint8')\n",
    "    img = FT.to_pil_image(ndarray)\n",
    "\n",
    "    # Random horizontal flip\n",
    "    if random.random() < 0.5:\n",
    "        img = FT.hflip(img)\n",
    "\n",
    "    # Random vertical flip\n",
    "    if random.random() < 0.5:\n",
    "        img = FT.vflip(img)\n",
    "\n",
    "    # Random rotation\n",
    "    angle = random.uniform(-30, 30)\n",
    "    img = FT.rotate(img, angle)\n",
    "\n",
    "\n",
    "    # Random color jitter\n",
    "    brightness = random.uniform(0.5, 1.5)\n",
    "    contrast = random.uniform(0.5, 1.5)\n",
    "    saturation = random.uniform(0.5, 1.5)\n",
    "    hue = random.uniform(-0.1, 0.1)\n",
    "    img = FT.adjust_brightness(img, brightness)\n",
    "    img = FT.adjust_contrast(img, contrast)\n",
    "    img = FT.adjust_saturation(img, saturation)\n",
    "    img = FT.adjust_hue(img, hue)\n",
    "\n",
    "    # Convert PIL Image back to ndarray\n",
    "    ndarray = FT.to_tensor(img)\n",
    "\n",
    "    return ndarray\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82a9760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "import random\n",
    "\n",
    "class ImageDatasetLoader(Dataset):\n",
    "    def __init__(self, mode='train', transformers=None, test_indices=[17000,17010,17020,17030,17040,17060,17070,17080,17090,17100,\n",
    "                                                                      18000,18100,18200,18300,18400,18500,18600,18700,18800,18900, \n",
    "                                                                      19000,19050,19150,19250,19350,19450,19550,19650,19750,19850,\n",
    "                                                                      19950,20050,20150,20250,20350,20450,20550,20650,20750,20850,\n",
    "                                                                      20950,22030,22130,22230,22330,22430,22530,22630,22730,22830,\n",
    "                                                                      23070,23170,23270,23370,23470,23570,23670,23770,23870,23970,\n",
    "                                                                     ]):\n",
    "        self.mode = mode\n",
    "        self.transform = transformers\n",
    "        \n",
    "        # Assuming you have the complete dataset in the 'data_load' variable\n",
    "        total_samples = len(data_load)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.files_CBCT = data_load[0:17000, 0]\n",
    "            self.files_sim = data_load[0:17000, 1]\n",
    "        elif mode == 'train1':\n",
    "            self.files_CBCT = data_load[0:17000, 0]\n",
    "            self.files_sim = data_load[0:17000, 1]\n",
    "        elif mode == 'test':\n",
    "            if test_indices is None:\n",
    "                # Randomly select 60 samples for the test set\n",
    "                test_indices = random.sample(range(17000, total_samples), 60)\n",
    "            self.files_CBCT = data_load[test_indices, 0]\n",
    "            self.files_sim = data_load[test_indices, 1]\n",
    "        elif mode == 'fid':\n",
    "            self.files_CBCT = data_load[21704:24704, 0]\n",
    "            self.files_sim = data_load[21704:24704, 1]\n",
    "        elif mode == 'evl':\n",
    "            if test_indices is None:\n",
    "                # Randomly select 60 samples for the test set\n",
    "                test_indices = random.sample(range(17000, total_samples), 60)\n",
    "            self.files_CBCT = data_load[test_indices, 0]\n",
    "            self.files_sim = data_load[test_indices, 1]\n",
    "\n",
    "        self.cbctnorm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.simctnorm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.alltransform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_sim)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_A = self.files_CBCT[index]\n",
    "        file_B = self.files_sim[index]\n",
    "        if self.transform is not None:\n",
    "            if self.mode == 'train': \n",
    "                img_A = self.alltransform(file_A)\n",
    "                img_B = self.alltransform(file_B)\n",
    "                file_cat = torch.cat((img_A, img_B, img_B), dim=0)\n",
    "                img_cat = self.transform(file_cat)\n",
    "                img_A, img_B, xx = torch.split(img_cat, [1, 1, 1], dim=0)\n",
    "                img_A = self.cbctnorm(file_A)\n",
    "                img_B = self.simctnorm(file_B)\n",
    "            else:\n",
    "                img_A = self.cbctnorm(file_A)\n",
    "                img_B = self.simctnorm(file_B)\n",
    "        return img_A, img_B\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam,AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the custom transform\n",
    "class NumpyToPIL(object):\n",
    "    def __call__(self, img):\n",
    "        return Image.fromarray(np.uint8(img))\n",
    "# Define your transforms\n",
    "data_transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.CenterCrop(size=(350, 350)),  # crop the center part of the image with size 300x300\n",
    "    # transforms.Resize(size=(384, 384)),  # resize the cropped image to 384x384\n",
    "    transforms.RandomHorizontalFlip(),  # randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomVerticalFlip(),  # randomly flip the image vertically with a probability of 0.5\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.2),  # randomly adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomRotation(degrees=15),  # randomly rotate the image by up to 15 degrees\n",
    "    # transforms.RandomCrop(size=(300, /300)),  # randomly crop the image to size 300x300\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # randomly apply Gaussian blur\n",
    "    # transforms.ToTensor(),  # convert the image to a PyTorch tensor\n",
    "    # transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])\n",
    "transforms_ = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1258bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam,AdamW\n",
    "from tqdm import tqdm\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "13e18e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab28569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.SiLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "    \n",
    "#next generation residual block\n",
    "class rSoftMax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super().__init__()\n",
    "        assert radix > 0\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Splat(nn.Module):\n",
    "    def __init__(self, channels, radix, cardinality, reduction_factor=4):\n",
    "        super(Splat, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "        self.channels = channels\n",
    "        inter_channels = max(channels*radix//reduction_factor, 32)\n",
    "        self.fc1 = nn.Conv2d(channels//radix, inter_channels, 1, groups=cardinality)\n",
    "#         self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.bn1 = nn.InstanceNorm2d(inter_channels)\n",
    "        self.relu = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(inter_channels, channels*radix, 1, groups=cardinality)\n",
    "        self.rsoftmax = rSoftMax(radix, cardinality)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, rchannel = x.shape[:2]\n",
    "        if self.radix > 1:\n",
    "            splited = torch.split(x, rchannel//self.radix, dim=1)\n",
    "            gap = sum(splited) \n",
    "        else:\n",
    "            gap = x\n",
    "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
    "        gap = self.fc1(gap)\n",
    "\n",
    "        gap = self.bn1(gap)\n",
    "        gap = self.relu(gap)\n",
    "\n",
    "        atten = self.fc2(gap)\n",
    "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
    "\n",
    "        if self.radix > 1:\n",
    "            attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
    "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
    "        else:\n",
    "            out = atten * x\n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70c8ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds_conv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, \n",
    "                 dilation=[1,3,5], groups=1, bias=True, \n",
    "                 act_layer='nn.SiLU(True)', init='kaiming'):\n",
    "        super().__init__()\n",
    "        assert in_planes%groups==0\n",
    "        assert kernel_size==3, 'only support kernel size 3 now'\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.with_bias = bias\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_planes, in_planes//groups, kernel_size, kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_planes))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.act = eval(act_layer)\n",
    "        self.init = init\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        if self.init == 'dirac':\n",
    "            nn.init.dirac_(self.weight, self.groups)\n",
    "        elif self.init == 'kaiming':\n",
    "            nn.init.kaiming_uniform_(self.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if self.with_bias:\n",
    "            if self.init == 'dirac':\n",
    "                nn.init.constant_(self.bias, 0.)\n",
    "            elif self.init == 'kaiming':\n",
    "                bound = self.groups / (self.kernel_size**2 * self.in_planes)\n",
    "                bound = math.sqrt(bound)\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = 0\n",
    "        for dil in self.dilation:\n",
    "            output += self.act(\n",
    "                F.conv2d(\n",
    "                    x, weight=self.weight, bias=self.bias, stride=self.stride, padding=dil,\n",
    "                    dilation=dil, groups=self.groups,\n",
    "                )\n",
    "            )\n",
    "        return output\n",
    "class AtrousSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, num_heads=2):\n",
    "        super(AtrousSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_channels // num_heads\n",
    "        self.query_conv = ds_conv2d(in_channels, out_channels, kernel_size)\n",
    "#         self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels,1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    " \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.query_conv.weight)\n",
    "        nn.init.kaiming_uniform_(self.key_conv.weight)\n",
    "        nn.init.kaiming_uniform_(self.value_conv.weight)\n",
    "        nn.init.zeros_(self.query_conv.bias)\n",
    "        nn.init.zeros_(self.key_conv.bias)\n",
    "        nn.init.zeros_(self.value_conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        queries = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "        keys = self.key_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 2, 3)\n",
    "        values = self.value_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "        energy = torch.matmul(queries, keys)\n",
    "        attention = torch.softmax(energy / self.head_dim**0.5, dim=-1)\n",
    "#         attention = self.dropout(attention)\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, height, width)\n",
    "        return out\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None, act_layer=nn.GELU,\n",
    "                 drop=0.2, with_depconv=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.with_depconv = with_depconv\n",
    "        \n",
    "        if self.with_depconv:\n",
    "            self.fc1 = nn.Conv2d(\n",
    "                in_features, hidden_features, 1, stride=1, padding=0, dilation=1, \n",
    "                groups=1, bias=True,\n",
    "            )\n",
    "            self.depconv = nn.Conv2d(\n",
    "                hidden_features, hidden_features, 3, stride=1, padding=1, dilation=1, \n",
    "                groups=hidden_features, bias=True,\n",
    "            )\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Conv2d(\n",
    "                hidden_features, out_features, 1, stride=1, padding=0, dilation=1, \n",
    "                groups=1, bias=True,\n",
    "            )\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.with_depconv:\n",
    "            x = self.fc1(x)\n",
    "            x = self.depconv(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6fdb7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class CSA(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, kernel_size=3, padding=1, stride=2,\n",
    "                 qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        \n",
    "        self.attn = nn.Linear(in_dim, kernel_size**4 * num_heads)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)\n",
    "        \n",
    "        self.csa_group = 1\n",
    "        assert out_dim % self.csa_group == 0\n",
    "        self.weight = nn.Conv2d(\n",
    "            self.kernel_size*self.kernel_size*out_dim, \n",
    "            self.kernel_size*self.kernel_size*out_dim, \n",
    "            1, \n",
    "            stride=1, padding=0, dilation=1, \n",
    "            groups=self.kernel_size*self.kernel_size*self.csa_group, \n",
    "            bias=qkv_bias,\n",
    "        )\n",
    "        assert qkv_bias == False\n",
    "        fan_out = self.kernel_size*self.kernel_size*self.out_dim\n",
    "        fan_out //= self.csa_group\n",
    "        self.weight.weight.data.normal_(0, math.sqrt(2.0 / fan_out)) # init\n",
    "        \n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x, v=None):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        B, H, W, _ = x.shape\n",
    "        h, w = math.ceil(H / self.stride), math.ceil(W / self.stride)\n",
    "        \n",
    "        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "        attn = self.attn(attn).reshape(\n",
    "            B, h * w, self.num_heads, self.kernel_size * self.kernel_size,\n",
    "            self.kernel_size * self.kernel_size).permute(0, 2, 1, 3, 4) # B,H,N,kxk,kxk\n",
    "        attn = attn * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        v = x.permute(0, 3, 1, 2) # B,C,H, W\n",
    "        v = self.unfold(v).reshape(\n",
    "            B, self.out_dim, self.kernel_size*self.kernel_size, h*w\n",
    "        ).permute(0,3,2,1).reshape(B*h*w, self.kernel_size*self.kernel_size*self.out_dim, 1, 1)\n",
    "        v = self.weight(v)\n",
    "        v = v.reshape(B, h*w, self.kernel_size*self.kernel_size, self.num_heads, \n",
    "                      self.out_dim//self.num_heads).permute(0,3,1,2,4).contiguous() # B,H,N,kxk,C/H\n",
    "        \n",
    "        x = (attn @ v).permute(0, 1, 4, 3, 2)\n",
    "        x = x.reshape(B, self.out_dim * self.kernel_size * self.kernel_size, h * w)\n",
    "        x = F.fold(x, output_size=(H, W), kernel_size=self.kernel_size, \n",
    "                   padding=self.padding, stride=self.stride)\n",
    "\n",
    "        x = self.proj(x.permute(0, 2, 3, 1))\n",
    "        x = self.proj_drop(x)\n",
    "        return x.permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fef769c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# class cross_attention(nn.Module):\n",
    "#     def __init__(self, in_channels,num_heads=2, kernel_size=3, dilation=1):\n",
    "#         super(cross_attention, self).__init__()\n",
    "#         out_channels = in_channels\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = out_channels // num_heads\n",
    "#         self.query_conv = nn.Conv2d(in_channels, out_channels,3,1,1)\n",
    "# #         self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.key_conv = nn.Conv2d(in_channels, out_channels,1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "# #         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         nn.init.kaiming_uniform_(self.query_conv.weight)\n",
    "#         nn.init.kaiming_uniform_(self.key_conv.weight)\n",
    "#         nn.init.kaiming_uniform_(self.value_conv.weight)\n",
    "#         nn.init.zeros_(self.query_conv.bias)\n",
    "#         nn.init.zeros_(self.key_conv.bias)\n",
    "#         nn.init.zeros_(self.value_conv.bias)\n",
    "\n",
    "#     def forward(self, x, k, v):\n",
    "#         batch_size, channels, height, width = x.size()\n",
    "#         queries = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "#         keys = self.key_conv(k).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 2, 3)\n",
    "#         values = self.value_conv(v).view(batch_size, self.num_heads, height*width, self.head_dim)\n",
    "#         energy = torch.matmul(queries, keys)\n",
    "#         attention = torch.softmax(energy / self.head_dim**0.5, dim=-1)\n",
    "#         out = torch.matmul(attention, values)\n",
    "#         out = out.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, height, width)\n",
    "#         return out\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self,in_channels) :\n",
    "        super(cross_attention, self).__init__()\n",
    "        self.queryConv =nn.Sequential(nn.Conv2d(in_channels, in_channels,1,1), \n",
    "                                      nn.GroupNorm(in_channels//4,in_channels)) \n",
    "        self.keyConv = nn.Sequential(nn.Conv2d(in_channels,in_channels,1,1), \n",
    "                                     nn.GroupNorm(in_channels//4,in_channels)) \n",
    "        self.psiConv = nn.Sequential(\n",
    "            nn.GroupNorm(in_channels//4,in_channels),\n",
    "            nn.Conv2d(in_channels,1,1,1), \n",
    "            nn.SiLU()) \n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        key = self.keyConv(key)\n",
    "        query = self.queryConv(query)\n",
    "        psi = F.silu(key+query)\n",
    "        psi = self.psiConv(psi)\n",
    "        return value*psi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfe21ad7-34ca-423e-9748-72d683b09348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(SPADE, self).__init__()\n",
    "        self.norm = nn.GroupNorm(in_channels//4,in_channels)\n",
    "        self.share = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "        self.conv_gamma = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "        self.conv_beta = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x, segmap):\n",
    "        normalized = self.norm(x)\n",
    "        seg_share  = self.share(F.silu(self.norm(segmap)))\n",
    "        gamma = self.conv_gamma(F.silu(self.norm(seg_share)))\n",
    "        #beta = F.silu(self.conv_beta(self.norm(x)))\n",
    "        return (1 + gamma) * normalized #+ beta\n",
    "\n",
    "class style_encoder(nn.Module):\n",
    "    def __init__(self,in_channels,embed_dim ):\n",
    "        super(style_encoder,self).__init__()\n",
    "        self.embed_dim =embed_dim \n",
    "        self.styleconv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleconv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "        self.styleconv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleconv4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*4),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.upUpsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.styleup4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*6,self.embed_dim*2, 3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleup3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*3,self.embed_dim*1, 3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim*1),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleup2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2,self.embed_dim*1,3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim*1),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward (self,x):\n",
    "        x1 = self.styleconv1(x)\n",
    "        x2 = self.styleconv2(x1)\n",
    "        x3 = self.styleconv3(x2)\n",
    "        x4 = self.styleconv4(x3)\n",
    "        \n",
    "        x5 = self.upUpsample(x4)\n",
    "        x5 = torch.cat((x5,x3),dim=1)\n",
    "        x5 = self.styleup4(x5)\n",
    "        \n",
    "        x6 = self.upUpsample(x5)\n",
    "        x6 = torch.cat((x6,x2),dim=1)\n",
    "        x6 = self.styleup3(x6)\n",
    "        \n",
    "        x7 = self.upUpsample(x6)\n",
    "        x7 = torch.cat((x7,x1),dim=1)\n",
    "        x7 = self.styleup2(x7)\n",
    "        \n",
    "        return x7, x6, x5, x4\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2cc5adc-5b2e-4380-9f2f-0988e2914e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 -1\n",
    "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channel))\n",
    "        if relu:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, norm=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            BasicConv(in_channel, out_channel, kernel_size=3, stride=1, norm=norm, relu=True),\n",
    "            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, norm=norm, relu=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "506b44c9-8f31-4779-882a-b12520423829",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class EBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, num_res=8, norm=False, first=False):\n",
    "        super(EBlock, self).__init__()\n",
    "        if first:\n",
    "            layers = [BasicConv(in_channel, out_channel, kernel_size=3, norm=norm, relu=True, stride=1)]\n",
    "        else:\n",
    "            layers = [BasicConv(in_channel, out_channel, kernel_size=3, norm=norm, relu=True, stride=2)]\n",
    "\n",
    "        layers += [ResBlock(out_channel, out_channel, norm) for _ in range(num_res)]\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class DBlock(nn.Module):\n",
    "    def __init__(self, channel, num_res=8, norm=False, last=False, feature_ensemble=False):\n",
    "        super(DBlock, self).__init__()\n",
    "\n",
    "        layers = [ResBlock(channel, channel, norm) for _ in range(num_res)]\n",
    "\n",
    "        if last:\n",
    "            if feature_ensemble == False:\n",
    "                layers.append(BasicConv(channel, 3, kernel_size=3, norm=norm, relu=False, stride=1))\n",
    "        else:\n",
    "            layers.append(BasicConv(channel, channel // 2, kernel_size=4, norm=norm, relu=True, stride=2, transpose=True))\n",
    "\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)\n",
    "\n",
    "\n",
    "class FOrD_v1(nn.Module):\n",
    "    def __init__(self, channel, rot_opt=False):\n",
    "        super(FOrD_v1, self).__init__()\n",
    "\n",
    "        self.decomp = BasicConv(channel, channel, kernel_size=1, relu=False, stride=1) \n",
    "\n",
    "    def forward(self, x):\n",
    "        x_decomp1 = self.decomp(x)\n",
    "        x_decomp1_norm = F.normalize(x_decomp1, p=2, dim=1)\n",
    "        x_decomp2 = x - torch.unsqueeze(torch.sum(x * x_decomp1_norm, dim=1), 1) * x_decomp1_norm\n",
    "\n",
    "        if rot_opt:\n",
    "            x_decomp2 = x_decomp2.transpose(2, 3).flip(2)\n",
    "\n",
    "        return x_decomp1, x_decomp2\n",
    "\n",
    "\n",
    "class XYDeblur(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(XYDeblur, self).__init__()\n",
    "\n",
    "        in_channel = 1\n",
    "        base_channel = 32\n",
    "        \n",
    "        num_res_ENC = 4\n",
    "\n",
    "        self.Encoder1 = EBlock(in_channel, base_channel, num_res_ENC, first=True)\n",
    "        self.Encoder2 = EBlock(base_channel, base_channel*2, num_res_ENC, norm=False)\n",
    "        self.Encoder3 = EBlock(base_channel*2, base_channel*4, num_res_ENC, norm=False)\n",
    "\n",
    "        self.Convs1_1 = BasicConv(base_channel * 4, base_channel * 2, kernel_size=1, relu=True, stride=1)\n",
    "        self.Convs1_2 = BasicConv(base_channel * 2, base_channel, kernel_size=1, relu=True, stride=1)\n",
    "\n",
    "        num_res_DEC = 4\n",
    "\n",
    "        self.Decoder1_1 = DBlock(base_channel * 4, num_res_DEC, norm=False)\n",
    "        self.Decoder1_2 = DBlock(base_channel * 2, num_res_DEC, norm=False)\n",
    "        self.Decoder1_3 = DBlock(base_channel, num_res_DEC, last=True, feature_ensemble=True)\n",
    "        self.Decoder1_4 = BasicConv(base_channel, 1, kernel_size=3, relu=False, stride=1)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        # Common encoder\n",
    "        x_e1 = self.Encoder1(x)\n",
    "        x_e2 = self.Encoder2(x_e1)\n",
    "        x_decomp = self.Encoder3(x_e2)\n",
    "\n",
    "        # Resultant image reconstruction\n",
    "        x_decomp1 = self.Decoder1_1(x_decomp)\n",
    "        x_decomp1 = self.Convs1_1(torch.cat([x_decomp1, x_e2], dim=1))\n",
    "        x_decomp1 = self.Decoder1_2(x_decomp1)\n",
    "        x_decomp1 = self.Convs1_2(torch.cat([x_decomp1, x_e1], dim=1))\n",
    "        x_decomp1 = self.Decoder1_3(x_decomp1)\n",
    "        x_decomp1 = self.Decoder1_4(x_decomp1)\n",
    "\n",
    "        x_decomp_rot = x_decomp.transpose(2, 3).flip(2)\n",
    "        x_e1_rot = x_e1.transpose(2, 3).flip(2)\n",
    "        x_e2_rot = x_e2.transpose(2, 3).flip(2)\n",
    "\n",
    "        x_decomp2 = self.Decoder1_1(x_decomp_rot)\n",
    "        x_decomp2 = self.Convs1_1(torch.cat([x_decomp2, x_e2_rot], dim=1))\n",
    "        x_decomp2 = self.Decoder1_2(x_decomp2)\n",
    "        x_decomp2 = self.Convs1_2(torch.cat([x_decomp2, x_e1_rot], dim=1))\n",
    "        x_decomp2 = self.Decoder1_3(x_decomp2)\n",
    "        x_decomp2 = self.Decoder1_4(x_decomp2)\n",
    "\n",
    "        x_decomp2 = x_decomp2.transpose(2, 3).flip(3)\n",
    "        \n",
    "        x_final = x_decomp1 + x_decomp2 + x\n",
    "        \n",
    "        return x_final\n",
    "\n",
    "def build_net():\n",
    "    return XYDeblur()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d37c1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.overlappatch_size=48\n",
    "        self.patch_size = 96\n",
    "        self.num_patch=(384//self.patch_size)\n",
    "        self.num_patches=self.num_patch**2\n",
    "        self.embed_dim = 64\n",
    "        \n",
    "        \n",
    "        self.patches = nn.Sequential(\n",
    "            nn.Conv2d(1, self.embed_dim,  kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim,  kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # self.cross_attention1 = cross_attention(in_channels=self.embed_dim, num_heads=2)\n",
    "        self.cross_attention1 = cross_attention(in_channels=self.embed_dim)\n",
    "        self.attn1 = CSA(in_dim=self.embed_dim, out_dim=self.embed_dim, num_heads=2)\n",
    "        self.pos_embed1 = nn.Parameter(torch.zeros(1, self.embed_dim, self.patch_size,self.patch_size))\n",
    "        self.norm1 = nn.GroupNorm(self.embed_dim//4,self.embed_dim)\n",
    "        self.mlp1 = Mlp(self.embed_dim,self.embed_dim*4)\n",
    "        # self.ResidualBlock1 = ResidualBlock(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim*2,  kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention2 = cross_attention(in_channels=self.embed_dim*2, num_heads=2)\n",
    "        self.cross_attention2 = cross_attention(in_channels=self.embed_dim*2)\n",
    "        self.attn2 = AtrousSelfAttention(self.embed_dim*2, self.embed_dim*2, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed2 = nn.Parameter(torch.zeros(1, self.embed_dim*2, self.patch_size//2, self.patch_size//2))\n",
    "        self.norm2 =nn.GroupNorm(32,self.embed_dim*2)\n",
    "        self.mlp2 = Mlp(self.embed_dim*2,self.embed_dim*2*8)\n",
    "        # self.ResidualBlock2 = ResidualBlock(self.embed_dim*2,self.embed_dim*2)\n",
    "\n",
    "        \n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*4, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(32,self.embed_dim*4),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention3 = cross_attention(in_channels=self.embed_dim*4, num_heads=2)\n",
    "        self.cross_attention3 = cross_attention(in_channels=self.embed_dim*4)\n",
    "        self.attn3 = AtrousSelfAttention(self.embed_dim*4, self.embed_dim*4, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed3 = nn.Parameter(torch.zeros(1, self.embed_dim*4, self.patch_size//4, self.patch_size//4))\n",
    "        self.norm3 = nn.GroupNorm(32,self.embed_dim*4)\n",
    "        self.mlp3 = Mlp(self.embed_dim*4,self.embed_dim*4*4)\n",
    "\n",
    "        \n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*4, self.embed_dim*8,  kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention4 = cross_attention(in_channels=self.embed_dim*1, num_heads=2)\n",
    "        self.cross_attention4 = cross_attention(in_channels=self.embed_dim*1)\n",
    "        self.attn4 = AtrousSelfAttention(self.embed_dim*8, self.embed_dim*8, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed4 = nn.Parameter(torch.zeros(1, self.embed_dim*8, self.patch_size//8, self.patch_size//8))\n",
    "        self.norm4 = nn.GroupNorm(self.embed_dim//2,self.embed_dim*8)\n",
    "        self.mlp4 = Mlp(self.embed_dim*8,self.embed_dim*8*4)\n",
    "\n",
    "\n",
    "        self.upUpsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # self.threethree = SPADE(self.embed_dim*4, self.embed_dim*4)\n",
    "        self.con3 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.con33 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=1, bias=False),\n",
    "            # nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=1),\n",
    "             nn.Dropout(0.2)\n",
    "        )\n",
    "            \n",
    "        self.con2 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*4),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*4,self.embed_dim*2, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.Dropout(0.2)      \n",
    "        )\n",
    "        self.con22 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*12),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*12, self.embed_dim*2, kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "            \n",
    "        )\n",
    "\n",
    "\n",
    "        \n",
    "        #48->96\n",
    "\n",
    "        self.oneone =SPADE(self.embed_dim*1, self.embed_dim*1)\n",
    "        self.con11 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*10),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*10, self.embed_dim*1,kernel_size=3, padding=1, stride=1, bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        self.con1 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*2,self.embed_dim*1, kernel_size=1),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.con11 = nn.Conv2d(self.embed_dim*2,self.embed_dim*1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "\n",
    "        self.upup =  SPADE(self.embed_dim*1, self.embed_dim*1)\n",
    "        self.conupup =  nn.Conv2d(self.embed_dim*1,self.embed_dim*1, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.conupupnorm = nn.GroupNorm(self.embed_dim//4,self.embed_dim)\n",
    "        \n",
    "        self.transup = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*5),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*5, self.embed_dim*5,  kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conup = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*6),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*6,self.embed_dim*1, kernel_size=1, bias=True),\n",
    "        )\n",
    "        self.transout = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*1,kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(self.embed_dim*1, 1, 3,1,1, bias=False),\n",
    "            # nn.Tanh(),\n",
    "        )   \n",
    "        \n",
    "        self.gelu=nn.GELU()\n",
    "        drop_path=0.2\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.transformer_layer = 2\n",
    "        self.style = style_encoder(1,self.embed_dim)\n",
    "        \n",
    "    def forward(self,x,style):\n",
    "        \n",
    "        style1, style2, style3, style4 = self.style(style)\n",
    "        \n",
    "        # x_deblur = self.deblur(x)\n",
    "        #input 1*384*384-> 16*96*96\n",
    "        x1 = self.patches(x)\n",
    "        x2t = self.down1(x1)\n",
    "        #x2r = self.ResidualBlock1(x2t)\n",
    "        x2 = x2t + self.pos_embed1\n",
    "        for i in range(self.transformer_layer):\n",
    "            x2 = x2 + self.drop_path(self.attn1(self.norm1(x2)))\n",
    "            x2 = F.silu(x2)\n",
    "        x2 = x2 + self.drop_path(self.mlp1(self.norm1(x2)))\n",
    "        \n",
    "        #16*96*96 -> 64*48*48\n",
    "        x3t = self.down2(x2)\n",
    "        #x3r = self.ResidualBlock2(x3t)\n",
    "        x3 = x3t + self.pos_embed2\n",
    "        for i in range(self.transformer_layer):\n",
    "            x3 = x3 + self.drop_path(self.attn2(self.norm2(x3)))\n",
    "            x3 = F.silu(x3)\n",
    "        x3 = x3 + self.drop_path(self.mlp2(self.norm2(x3)))\n",
    "\n",
    "        \n",
    "        #64*48*48 ->256*24*24\n",
    "        x4t = self.down3(x3)\n",
    "        #x4r = self.ResidualBlock3(x4t)\n",
    "        x4 = x4t + self.pos_embed3\n",
    "        for i in range(self.transformer_layer):\n",
    "            x4 = x4 + self.drop_path(self.attn3(self.norm3(x4)))\n",
    "            x4 = F.silu(x4)\n",
    "        x4 = x4 + self.drop_path(self.mlp3(self.norm3(x4)))\n",
    "\n",
    "        x5t = self.down4(x4)\n",
    "        #x5r = self.ResidualBlock4(x5t)\n",
    "        x5 = x5t + self.pos_embed4\n",
    "        for i in range(self.transformer_layer):\n",
    "            x5 = x5 + self.drop_path(self.attn4(self.norm4(x5)))\n",
    "            x5 = F.silu(x5)\n",
    "        x5 = x5 + self.drop_path(self.mlp4(self.norm4(x5)))\n",
    "        \n",
    "        # x6 = self.upUpsample(x5)\n",
    "        x6 = self.con33(x5)\n",
    "        # x4_cross = self.cross_attention3(x6,x4,x4)\n",
    "        x4_cross = self.cross_attention3(x6,x4)\n",
    "        x6_cat = torch.cat((x6,x4_cross),dim=1)\n",
    "        x6_cat1 = self.con3(x6_cat)\n",
    "        x6_cat_cross = self.cross_attention3(x6_cat1,style4)\n",
    "        x6_cat = torch.cat((x6_cat1,x6_cat1,x6_cat_cross),dim=1)\n",
    "        # x6_cat = x6_cat1 +  F.silu(self.norm3(self.con33(x6_cat)))\n",
    "        \n",
    "        # x7 = self.upUpsample(x6_cat)\n",
    "        x7 = self.con22(x6_cat)\n",
    "        # x3_cross = self.cross_attention2(x7,x3,x3)\n",
    "        x3_cross = self.cross_attention2(x7,x3)\n",
    "        x7_cat = torch.cat((x7,x3_cross),dim=1)\n",
    "        x7_cat1 = self.con2(x7_cat)\n",
    "        # x7_cat_cross = self.cross_attention2(x7_cat1,style3,style3)\n",
    "        x7_cat_cross = self.cross_attention2(x7_cat1,style3)\n",
    "        x7_cat = torch.cat((x7_cat1,x7_cat1,x7_cat1,x7_cat_cross,x7_cat_cross),dim=1)\n",
    "        # x7_cat = x7_cat1 + F.silu(self.norm2(self.con22(x7_cat)))\n",
    "        \n",
    "        # x8 = self.upUpsample(x7_cat)\n",
    "        x8 = self.con11(x7_cat)\n",
    "        # x2_cross = self.cross_attention1(x8,x2,x2)\n",
    "        x2_cross = self.cross_attention1(x8,x2)\n",
    "        x8_cat = torch.cat((x8,x2_cross),dim=1)\n",
    "        x8_cat1 = self.con1(x8_cat)\n",
    "        # x8_cat_cross = self.cross_attention1(x8_cat1,style2,style2)\n",
    "        x8_cat_cross = self.cross_attention1(x8_cat1,style2)\n",
    "        x8_cat = torch.cat((x8_cat1,x8_cat1,x8_cat_cross,x8_cat_cross,x8_cat_cross),dim=1)\n",
    "        # x8_cat = x8_cat1 + F.silu(self.norm1(self.con11(x8_cat)))\n",
    "        \n",
    "        x9 = self.transup(x8_cat)\n",
    "        x9_cat = torch.cat((x9,x1),dim=1)\n",
    "        x9_cat = self.conup(x9_cat)\n",
    "        x9_cat_cross = self.cross_attention4(x9_cat,style1)\n",
    "        # x9_spade = x9_cat + (self.conupup(F.silu(self.conupupnorm(self.upup(F.silu(x9_cat),F.silu(style1))))))\n",
    "        # x9_cat = x9_cat + (self.conupup(F.silu(self.conupupnorm(self.upup(F.silu(x9_spade),F.silu(style1))))))\n",
    "        x9_cat1 = torch.cat((x9_cat,x9_cat_cross),dim=1)\n",
    "        \n",
    "        x11 = self.transout(x9_cat1)\n",
    "        x11 = self.output(x11)\n",
    "        return x11\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2c8b5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FPSE discriminator \n",
    "class TDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc=1, nf=64, label_nc=1):\n",
    "        super(TDiscriminator, self).__init__()\n",
    "        # bottom-up pathway\n",
    "        \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, nf, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(nf//4,nf),\n",
    "            nn.SiLU(True))\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*2, nf*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*8),\n",
    "            nn.SiLU(True))\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*8),\n",
    "            nn.SiLU(True))\n",
    "\n",
    "        # top-down pathway\n",
    "        self.lat2 = nn.Sequential(\n",
    "            nn.Conv2d(nf*2, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat5 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        \n",
    "        # upsampling\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # final layers\n",
    "        self.final2 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.final3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.final4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "    \n",
    "        # true/false prediction and semantic alignment prediction\n",
    "        self.tf = nn.Conv2d(nf*2, 1, kernel_size=1)\n",
    "        self.seg = nn.Conv2d(nf*2, nf*2, kernel_size=1)\n",
    "        self.embedding = nn.Conv2d(label_nc, nf*2, kernel_size=1)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake_and_real_img, segmap):\n",
    "        # bottom-up pathway\n",
    "        feat11 = self.enc1(fake_and_real_img)  #384->192\n",
    "        feat12 = self.enc2(feat11)   #192->96\n",
    "        feat13 = self.enc3(feat12)   #96->48\n",
    "        feat14 = self.enc4(feat13)  #48->24\n",
    "        feat15 = self.enc5(feat14)   #24->12\n",
    "        # top-down pathway and lateral connections\n",
    "        feat25 = self.lat5(feat15)\n",
    "        feat24 = self.up(feat25) + self.lat4(feat14)  #24\n",
    "        feat23 = self.up(feat24) + self.lat3(feat13)  #48\n",
    "        feat22 = self.up(feat23) + self.lat2(feat12)  #96\n",
    "        # final prediction layers\n",
    "        feat32 = self.final2(feat22)\n",
    "        feat33 = self.final3(feat23)\n",
    "        feat34 = self.final4(feat24)\n",
    "        # Patch-based True/False prediction\n",
    "        pred2 = self.tf(feat32)\n",
    "        pred3 = self.tf(feat33)\n",
    "        pred4 = self.tf(feat34)\n",
    "        seg2 = self.seg(feat32)\n",
    "        seg3 = self.seg(feat33)\n",
    "        seg4 = self.seg(feat34)\n",
    "\n",
    "        # intermediate features for discriminator feature matching loss\n",
    "        feats = [feat12, feat13, feat14, feat15]\n",
    "\n",
    "        # segmentation map embedding\n",
    "        segemb = self.embedding(segmap)\n",
    "        segemb = F.avg_pool2d(segemb, kernel_size=2, stride=2)\n",
    "        segemb2 = F.avg_pool2d(segemb, kernel_size=2, stride=2)\n",
    "        segemb3 = F.avg_pool2d(segemb2, kernel_size=2, stride=2)\n",
    "        segemb4 = F.avg_pool2d(segemb3, kernel_size=2, stride=2)\n",
    "\n",
    "        # semantics embedding discriminator score\n",
    "        pred2 += torch.mul(segemb2, seg2).sum(dim=1, keepdim=True)\n",
    "        pred3 += torch.mul(segemb3, seg3).sum(dim=1, keepdim=True)\n",
    "        pred4 += torch.mul(segemb4, seg4).sum(dim=1, keepdim=True)\n",
    "\n",
    "        # concat results from multiple resolutions\n",
    "        results = [pred2, pred3, pred4]\n",
    "            \n",
    "\n",
    "        return [feats, results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9e71a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientVariance(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size):\n",
    "        super(GradientVariance, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # Sobel kernel for the gradient map calculation\n",
    "        self.kernel_x = torch.FloatTensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).unsqueeze(0).unsqueeze(0).cuda()\n",
    "        self.kernel_y = torch.FloatTensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).unsqueeze(0).unsqueeze(0).cuda()\n",
    "        # operation for unfolding image into non overlapping patches\n",
    "        self.unfold = torch.nn.Unfold(kernel_size=(self.patch_size, self.patch_size), stride=self.patch_size)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # calculation of the gradient maps of x and y directions\n",
    "        gx_target = F.conv2d(target, self.kernel_x, stride=1, padding=1)\n",
    "        gy_target = F.conv2d(target, self.kernel_y, stride=1, padding=1)\n",
    "        gx_output = F.conv2d(output, self.kernel_x, stride=1, padding=1)\n",
    "        gy_output = F.conv2d(output, self.kernel_y, stride=1, padding=1)\n",
    "\n",
    "        # unfolding image to patches\n",
    "        gx_target_patches = self.unfold(gx_target)\n",
    "        gy_target_patches = self.unfold(gy_target)\n",
    "        gx_output_patches = self.unfold(gx_output)\n",
    "        gy_output_patches = self.unfold(gy_output)\n",
    "\n",
    "        # calculation of variance of each patch\n",
    "        var_target_x = torch.var(gx_target_patches, dim=1)\n",
    "        var_output_x = torch.var(gx_output_patches, dim=1)\n",
    "        var_target_y = torch.var(gy_target_patches, dim=1)\n",
    "        var_output_y = torch.var(gy_output_patches, dim=1)\n",
    "\n",
    "        # loss function as a MSE between variances of patches extracted from gradient maps\n",
    "        gradvar_loss = F.mse_loss(var_target_x, var_output_x) + F.mse_loss(var_target_y, var_output_y)\n",
    "\n",
    "        return gradvar_loss\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure,MetricCollection\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda else torch.Tensor\n",
    "def sample_images(real_A, real_B,G_AB,G_BA,epoch,figside=1.5,):\n",
    "    assert real_A.size() == real_B.size(), 'The image size for two domains must be the same'\n",
    "    \n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    real_A = real_A.type(Tensor)\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    real_B = real_B.type(Tensor)\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "    psnrA = PeakSignalNoiseRatio(data_range=torch.max(real_A)-torch.min(real_A)).cuda()\n",
    "    psnrB = PeakSignalNoiseRatio(data_range=torch.max(real_B)-torch.min(real_B)).cuda()\n",
    "    ssim = StructuralSimilarityIndexMeasure().cuda()\n",
    "\n",
    "\n",
    "    ans_ssim=ssim(fake_B,real_A)\n",
    "    ans_psnr_toB=psnrB(fake_B,real_B)\n",
    "    ans_psnr_toA=psnrA(fake_B,real_A)\n",
    "    \n",
    "    nrows = real_A.size(0)\n",
    "    real_A = make_grid(real_A, nrow=nrows, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=nrows, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=nrows, normalize=True)\n",
    "    \n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1).cpu().permute(1, 2, 0)\n",
    "    \n",
    "    plt.figure(figsize=(figside*nrows, figside*4))\n",
    "    plt.imshow(image_grid)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'epoch{epoch}plot.png')\n",
    "    plt.show()\n",
    "    plt.close(image_grid)\n",
    "    return ans_ssim,ans_psnr_toB,ans_psnr_toA\n",
    "\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "class MS_SSIM_Loss2(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1*( 1 - super(MS_SSIM_Loss2, self).forward(img1, img2) )\n",
    "MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "class CustomSSIMLoss(SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 * (1 - super(CustomSSIMLoss, self).forward(img1, img2))\n",
    "\n",
    "SSIM_Loss = CustomSSIMLoss(data_range=1.0, size_average=True, channel=1)\n",
    "def unnormalize(sample_dicom, x):\n",
    "    ds = sample_dicom.copy()\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "    ymin = -200\n",
    "    ymax = 1600\n",
    "    pixel_min = ds.pixel_array.min() #/ slope + intercept\n",
    "    pixel_max = ds.pixel_array.max() #/ slope + intercept\n",
    "    # normalized_array = np.clip((x + 1) , 0, 2)\n",
    "#     mean = np.mean(x)\n",
    "#     std = np.std(x)\n",
    "#     unnormalized_array = (x * std + mean) * slope + intercept\n",
    "    scaled_array = (x)*10-400\n",
    "    unnormalized_array = (scaled_array - intercept) / slope \n",
    "    # unnormalized_array = unnormalized_array*(600)-300\n",
    "    return unnormalized_array\n",
    "\n",
    "def write_dicom(sample_dicom, array, paths, tt):\n",
    "    ds = sample_dicom.copy()\n",
    "    ww = ds.WindowWidth\n",
    "    wc = ds.WindowCenter\n",
    "    array = np.transpose(array, (2, 1, 0))[:, :, 0]\n",
    "    array = unnormalize(sample_dicom, array)\n",
    "    array = np.clip(array, 0, 2 ** 16 - 1)\n",
    "    ds.PixelData = array.astype(np.uint16).tobytes()\n",
    "    return ds.save_as(paths + \"/\" + str(tt) + \".dcm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba1972e9-1e48-4c8d-9cdf-9d8ccbed9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0 NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# all loss function here\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "\n",
    "class MS_SSIM_Loss2(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1*( 1 - super(MS_SSIM_Loss2, self).forward(img1, img2) )\n",
    "MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "class CustomSSIMLoss(SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 * (1 - super(CustomSSIMLoss, self).forward(img1, img2))\n",
    "\n",
    "SSIM_Loss = CustomSSIMLoss(data_range=1.0, size_average=True, channel=1)\n",
    "# import torch.autograd as autograd\n",
    "class GDL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GDL, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_dy = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])\n",
    "        pred_dx = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])\n",
    "        target_dy = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])\n",
    "        target_dx = torch.abs(target[:, :, :, 1:] - target[:, :, :, :-1])\n",
    "        grad_diff_x = torch.abs(target_dx - pred_dx)\n",
    "        grad_diff_y = torch.abs(target_dy - pred_dy)\n",
    "        return grad_diff_x.mean() + grad_diff_y.mean()\n",
    "def sobel_sharpness_loss(generated_image, real_image):\n",
    "    sobel_x = torch.tensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).float().unsqueeze(0).unsqueeze(0)\n",
    "    sobel_y = torch.tensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).float().unsqueeze(0).unsqueeze(0)\n",
    "    if torch.cuda.is_available():\n",
    "        sobel_x = sobel_x.cuda()\n",
    "        sobel_y = sobel_y.cuda()\n",
    "    G_x = nn.functional.conv2d(generated_image, sobel_x)\n",
    "    G_y = nn.functional.conv2d(generated_image, sobel_y)\n",
    "    generated_edges = torch.sqrt(torch.pow(G_x, 2) + torch.pow(G_y, 2))\n",
    "    R_x = nn.functional.conv2d(real_image, sobel_x)\n",
    "    R_y = nn.functional.conv2d(real_image, sobel_y)\n",
    "    real_edges = torch.sqrt(torch.pow(R_x, 2) + torch.pow(R_y, 2))\n",
    "    loss = nn.L1Loss()(generated_edges, real_edges)\n",
    "    return loss\n",
    "def high_dose_sharp_l2loss(predicted, target, gamma=100):\n",
    "    loss = torch.mean((1 / (1 + torch.exp(-(target - 0.03) * gamma))) * torch.pow(predicted - target, 2))\n",
    "    return loss\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions,seg,real=True):\n",
    "        # Compute the hinge loss using Swish activation\n",
    "        loss = 0\n",
    "        if real ==True:\n",
    "            for pred in seg:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 - pred))\n",
    "            for preds in predictions:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 - preds))\n",
    "        else:\n",
    "            for pred in seg:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 + pred))\n",
    "            for preds in predictions:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 + preds))\n",
    "            \n",
    "        return loss\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torchvision\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "class Vgg19_out(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(Vgg19_out, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).to(device) #.cuda()\n",
    "        vgg.eval()\n",
    "        vgg_pretrained_features = vgg.features\n",
    "        #print(vgg_pretrained_features)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2): #(3):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7): #(3, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12): #(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21): #(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):#(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not self.requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    " \n",
    "    def forward(self, X):\n",
    "        h=self.input(X)\n",
    "        h_relu1 = self.slice1(h)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        h_relu5 = self.slice5(h_relu4)\n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out\n",
    "\n",
    "class Perceptual_loss134(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptual_loss134, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        self.vgg = Vgg19_out().to(device)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.weights = [1.0/7, 1.0/5, 1.0/3, 1.0/1, 1.0/7]\n",
    "        # self.weights = [1.0/2.6, 1.0/16, 1.0/2, 1.0/1, 1.0]    \n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        # for i in range(len(x_vgg)-2):\n",
    "        #     loss += self.weights[i] * self.mse(x_vgg[i], y_vgg[i].detach())    \n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.L1(x_vgg[i], y_vgg[i].detach())        \n",
    "        return loss\n",
    "class Perceptual_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptual_loss, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        self.vgg = Vgg19_out().to(device)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "        #self.weights = [1.0/2.6, 1.0/16, 1.0/3.7, 1.0/5.6, 1.0]\n",
    "        self.weights = [1.0/16, 1.0/8, 1.0/4, 1.0/2, 1.0/1]    \n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        # loss += self.weights[2] * self.L1(x_vgg[2], y_vgg[2].detach())\n",
    "        # loss += self.weights[4] * self.L1(x_vgg[4], y_vgg[4].detach())\n",
    "        # loss += self.weights[3] * self.mse(x_vgg[3], y_vgg[3].detach())\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.L1(x_vgg[i], y_vgg[i].detach()) \n",
    "        # for i in range(len(x_vgg)-1):\n",
    "        #     loss += self.weights[i] * self.MS_SSIM_Loss(x_vgg[i], y_vgg[i].detach())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def calculate_fid_score(generator, testloader, device=device):\n",
    "    fid = FrechetInceptionDistance(feature=2048)#.to(device)\n",
    "    gen_features = []\n",
    "    real_features = []\n",
    "    generated_list = None  # Initialize generated_list variable\n",
    "    images_list = None  # Initialize images_list variable\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (x_img, y_img) in enumerate(testloader):\n",
    "            y_img1 = y_img.type(torch.cuda.FloatTensor).to(device)\n",
    "            x_img1 = x_img.type(torch.cuda.FloatTensor).to(device)\n",
    "            generated_images = generator(x_img1, y_img1)\n",
    "            generated_images = F.interpolate(generated_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            generated_images_rgb = torch.cat([generated_images] * 3, dim=1)  # Convert grayscale to RGB\n",
    "            if generated_list is None:\n",
    "                generated_list = torch.cat((generated_images_rgb, generated_images_rgb), dim=0)\n",
    "            generated_list = torch.cat((generated_list, generated_images_rgb), dim=0)\n",
    "\n",
    "            images = y_img1\n",
    "            images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            images_rgb = torch.cat([images] * 3, dim=1)  # Convert grayscale to RGB\n",
    "            if images_list is None:\n",
    "                images_list = torch.cat((images_rgb, images_rgb), dim=0)\n",
    "            images_list = torch.cat((images_list, images_rgb), dim=0)\n",
    "\n",
    "    generated_list = generated_list.cpu().type(torch.uint8)  # Convert generated_list to torch.uint8\n",
    "    images_list = images_list.cpu().type(torch.uint8)  # Convert images_list to torch.uint8\n",
    "\n",
    "    fid.update(images_list, real=True)\n",
    "    fid.update(generated_list, real=False)\n",
    "    s= fid.compute()\n",
    "    return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "567c669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "class CycleGANsformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "        torch.cuda.set_device(0)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        self.discX = TDiscriminator().to(device)\n",
    "        self.device=device\n",
    "        self.genX2Y = TGenerator().to(device)\n",
    "        self.state_dict = {\n",
    "            'self.discX': self.discX.state_dict(),\n",
    "            'self.genX2Y': self.genX2Y.state_dict()\n",
    "        }\n",
    "        checkpoint_path = './0716lite_checkpoint/0716lite_epoch52.pth'\n",
    "        # Load the checkpoint\n",
    "        checkpoint = torch.load(checkpoint_path)\n",
    "        self.genX2Y.load_state_dict(checkpoint['self.genX2Y'])\n",
    "        self.discX.load_state_dict(checkpoint['self.discX'])\n",
    "\n",
    "        self.opt_discX = optim.AdamW(\n",
    "            self.discX.parameters(),\n",
    "            lr=7.5e-5,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "        self.opt_genX2Y = optim.AdamW(\n",
    "            self.genX2Y.parameters(),\n",
    "            lr=7.5e-5,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        self.opt_genX2Y_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.opt_genX2Y, mode='min', factor=0.5, patience=7, verbose=True)\n",
    "        self.opt_discX_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.opt_discX, mode='min', factor=0.5, patience=7, verbose=True)\n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.GradientDifferenceLoss = GDL()\n",
    "        self.HingeLoss = HingeLoss()\n",
    "        self.Perceptual_loss = Perceptual_loss134()\n",
    "        self.deblur_percept = Perceptual_loss()\n",
    "        self.batch_size=6\n",
    "    def fit(self, dataset, epochs=100):\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "        testloader=DataLoader(ImageDatasetLoader(mode=\"test\",transformers=transforms_ ), batch_size=self.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        \n",
    "        print(\"Start the training!\")\n",
    "        step = 0\n",
    "        for epoch in range(epochs):\n",
    "            for idx, (x_img, y_img) in enumerate(loader):\n",
    "                step+=1\n",
    "                y_img1=y_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                x_img1=x_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                self.genX2Y.train()\n",
    "                self.discX.train()\n",
    "#                 assert fake_x.size() == y_img1.size() , f\"fake_imgs.size(): {fake_x.size()} real_imgs.size(): {y_img1.size()}\"\n",
    "#######################################################################\n",
    "######################################################################                \n",
    "                self.opt_genX2Y.zero_grad()\n",
    "                fake_y1 = self.genX2Y(x_img1, y_img1)\n",
    "                D_Y_real1, segmap1 = self.discX(fake_y1, y_img1)\n",
    "                # D_Y_real1, segmap1 = self.discX(fake_y1[2], y_img1)\n",
    "            \n",
    "                generator_loss = self.HingeLoss(D_Y_real1,segmap1,real = False)\n",
    "                \n",
    "                fake_y2 = self.genX2Y(x_img1, y_img1)\n",
    "                perceptual_loss = self.Perceptual_loss(fake_y2,y_img1)\n",
    "                # perceptual_loss = self.Perceptual_loss(fake_y2[2],y_img1)\n",
    "                \n",
    "                cycle_X = self.genX2Y(x_img1, y_img1)\n",
    "                # GDL_loss = MS_SSIM_Loss(cycle_X,x_img1)*0.84 +  self.L1(cycle_X,x_img1)*0.16 #+ self.deblur_percept(cycle_X,x_img1)*0.7  #+ self.L1(cycle_X,x_img1)*0.05\n",
    "                deblur_percept_loss =  self.deblur_percept(cycle_X,x_img1)*1 + self.GradientDifferenceLoss(cycle_X,x_img1)*1 #+ self.L1(cycle_X,x_img1)*0.16# +  MS_SSIM_Loss(cycle_X,x_img1)*0.84 +  self.GradientDifferenceLoss(cycle_X,x_img1)*0.16 #self.deblur_percept(cycle_X,x_img1)\n",
    "                \n",
    "                G_loss = generator_loss*1 + perceptual_loss*0.198 + deblur_percept_loss *0.205               \n",
    "                   \n",
    "                G_loss.backward(retain_graph=True)\n",
    "                self.opt_genX2Y.step()\n",
    "\n",
    "##################################################                \n",
    "                self.opt_discX.zero_grad()\n",
    "                D_X_real,seg_map2 = self.discX(x_img1,y_img1)\n",
    "                # with torch.no_grad():\n",
    "                #     fake_x = self.genX2Y(x_img1,y_img1)\n",
    "                #     fake_x = fake_x.detach()\n",
    "                #     fake_x.requires_grad_()\n",
    "                fake_x = self.genX2Y(x_img1,y_img1)\n",
    "                D_X_fake,seg_map3 = self.discX(fake_x,y_img1)\n",
    "                # D_X_fake,seg_map3 = self.discX(fake_x[2],y_img1)\n",
    "                D_real_loss = self.HingeLoss(D_X_real,seg_map2)\n",
    "                D_fake_loss = self.HingeLoss(D_X_fake,seg_map3,real = False)\n",
    "                D_X_loss = D_real_loss*1 + D_fake_loss*1\n",
    "                D_X_loss.backward(retain_graph=True)\n",
    "                self.opt_discX.step()\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "            self.opt_genX2Y_scheduler.step(G_loss)\n",
    "            self.opt_discX_scheduler.step(D_X_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            dir_checkpoint =\"./0716lite_checkpoint/\"\n",
    "            torch.save(self.state_dict, dir_checkpoint + f'0716lite_epoch{epoch}.pth')\n",
    "            print(f'[Epoch {epoch+1}/{epochs}]')\n",
    "            print(f'[G loss: {G_loss.item()} | generator_loss: {generator_loss.item()} |perceptual_loss: {perceptual_loss.item()} |deblur_percept_loss: {deblur_percept_loss.item()}]')\n",
    "            print(f'[D loss: { D_X_loss.item()} ]')\n",
    "#             ttest_real_A, ttest_real_B = next(iter(testloader))\n",
    "#             ans_ssim,ans_psnr_toB,ans_psnr_toA=sample_images(ttest_real_A, ttest_real_B,self.genX2Y,self.genY2X)\n",
    "            if (epoch) % 1 == 0:\n",
    "                self.genX2Y.eval()\n",
    "                with torch.no_grad():\n",
    "                    ans_ssim = 0\n",
    "                    ans_psnr_toB = 0\n",
    "                    ans_psnr_toA = 0\n",
    "                    for idx, (x_img, y_img) in enumerate(testloader):\n",
    "                        y_img1=y_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                        x_img1=x_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                        psnrA = PeakSignalNoiseRatio(data_range=torch.max(x_img1)-torch.min(x_img1)).cuda()\n",
    "                        psnrB = PeakSignalNoiseRatio(data_range=torch.max(y_img1)-torch.min(y_img1)).cuda()\n",
    "                        ssim = StructuralSimilarityIndexMeasure().cuda()\n",
    "                        fake_B = self.genX2Y(x_img1,y_img1)\n",
    "                        # fake_B = self.deblur(fake_B)\n",
    "\n",
    "                        ans_ssim +=ssim(fake_B,x_img1)\n",
    "                        ans_psnr_toB +=psnrB(fake_B,y_img1)\n",
    "                        ans_psnr_toA +=psnrA(fake_B,x_img1)\n",
    "\n",
    "                        if idx == len(testloader)-1:\n",
    "\n",
    "                            nrows = x_img1.size(0)\n",
    "                            real_A = make_grid(x_img1, nrow=nrows, normalize=True)\n",
    "                            fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n",
    "                            real_B = make_grid(y_img1, nrow=nrows, normalize=True)\n",
    "\n",
    "                            image_grid = torch.cat((real_A, fake_B, real_B), 1).cpu().permute(1, 2, 0)\n",
    "\n",
    "                            plt.figure(figsize=(1.5*nrows, 1.5*3))\n",
    "                            plt.imshow(image_grid)\n",
    "                            plt.axis('off')\n",
    "                            plt.savefig(f'./0716lite_checkpoint/plot/{epoch}.png')\n",
    "                            plt.show()\n",
    "                            plt.close(image_grid)\n",
    "\n",
    "\n",
    "\n",
    "                    print(f\"Epoch {epoch} | ssim : {ans_ssim/len(testloader)}\")\n",
    "                    print(f\"Epoch {epoch} | psnr_toB : {ans_psnr_toB/len(testloader)}\")\n",
    "                    print(f\"Epoch {epoch} | psnr_toA : {ans_psnr_toA/len(testloader)}\") \n",
    "\n",
    "                    \n",
    "                    cbct_count=0\n",
    "                    path=\"./0716/\"+str(epoch)+\"/\"\n",
    "                    if not os.path.exists(path):\n",
    "                        os.mkdir(path)\n",
    "                    cbct_data=[]\n",
    "                    for i in range(len(data)):\n",
    "                        ds=pydicom.dcmread(data[i][0],force=True)\n",
    "                        cbct_data.append(ds)\n",
    "                    cbctdata=cbct_data[24600:24704]\n",
    "                    sum_ssim=0\n",
    "                    sum_ans_psnr_toB=0\n",
    "                    sum_ans_psnr_toA=0\n",
    "                    for idx, (a_img, b_img) in enumerate(testloader):\n",
    "                        a=a_img.type(torch.cuda.FloatTensor)\n",
    "                        b=b_img.type(torch.cuda.FloatTensor)\n",
    "\n",
    "                        fake_B = self.genX2Y(a,b)\n",
    "                        # fake_B = self.deblur(fake_B)\n",
    "                        b = fake_B.cpu().numpy()\n",
    "                        c =b.copy()\n",
    "                        for s in range(len(a_img)):\n",
    "                            t=s+idx*self.batch_size\n",
    "                            write_dicom(cbctdata[t],c[s],path,t)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.genX2Y(x) # get generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "93d96243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch.utils.data import ConcatDataset\n",
    "\n",
    "# cg = CycleGANsformer()\n",
    "# # cg.fit(ImageDatasetLoader(mode=\"train\",transformers=data_transform ))\n",
    "# seta = ImageDatasetLoader(mode=\"train1\",transformers=transforms_)\n",
    "# setb = ImageDatasetLoader(mode=\"train\",transformers=data_transform)\n",
    "# trainset = ConcatDataset([seta, setb])\n",
    "# cg.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1086c6",
   "metadata": {},
   "source": [
    "wwwwww"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "02c288a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = './0716lite_checkpoint/0716lite_epoch14.pth'\n",
    "\n",
    "# # Load the checkpoint\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "# torch.cuda.set_device(1)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# model = TGenerator().to(device)\n",
    "# # Load the model state_dict from the checkpoint\n",
    "# model.load_state_dict(checkpoint['self.genX2Y'])\n",
    "\n",
    "# # Optionally, load other components such as optimizer state_dict or epoch number\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# # epoch = checkpoint['epoch']\n",
    "\n",
    "# # Set the model in evaluation mode\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     fidloader=DataLoader(ImageDatasetLoader(mode=\"fid\",transformers=transforms_ ), batch_size=1, shuffle=False, num_workers=16)\n",
    "#     fretchet_dist=calculate_fid_score(model,fidloader) \n",
    "#     print( f\" fid : {fretchet_dist}\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11c17396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 1 NVIDIA A100-SXM4-80GB\n",
      "Execution time: 16.035374402999878 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "def unnormalize(sample_dicom, x):\n",
    "    ds = sample_dicom.copy()\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "    ymin = -200\n",
    "    ymax = 1600\n",
    "    pixel_min = ymin #/ slope + intercept\n",
    "    pixel_max = ymax #/ slope + intercept\n",
    "#     normalized_array = np.clip((x + 1) , 0, 2)\n",
    "#     mean = np.mean(x)\n",
    "#     std = np.std(x)\n",
    "#     unnormalized_array = (x * std + mean) * slope + intercept\n",
    "    scaled_array = (x)*6.5-600#(x*0.5+0.5)*100#-500\n",
    "    unnormalized_array = (scaled_array - intercept) / slope \n",
    "    return unnormalized_array\n",
    "\n",
    "\n",
    "def write_dicom(sample_dicom, array, paths, tt):\n",
    "    ds = sample_dicom.copy()\n",
    "    ww = ds.WindowWidth\n",
    "    wc = ds.WindowCenter\n",
    "    array = np.transpose(array, (2, 1, 0))[:, :, 0]\n",
    "    array = unnormalize(sample_dicom, array)\n",
    "    array = np.clip(array, 0, 2 ** 16 - 1)\n",
    "    ds.PixelData = array.astype(np.uint16).tobytes()\n",
    "    return ds.save_as(paths + \"/\" + str(tt) + \".dcm\")\n",
    "checkpoint_path = './0716lite_checkpoint/0716lite_epoch14.pth'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "model = TGenerator().to(device)\n",
    "# Load the model state_dict from the checkpoint\n",
    "model.load_state_dict(checkpoint['self.genX2Y'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cbct_count=0\n",
    "    batch_sizes = 2\n",
    "    testloader=DataLoader(ImageDatasetLoader(mode=\"test\",transformers=transforms_ ), batch_size=batch_sizes, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    evlloader=DataLoader(ImageDatasetLoader(mode=\"evl\",transformers=transforms_ ), batch_size=batch_sizes, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    path=\"./0716/\"+str(99)+\"/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    cbct_data=[]\n",
    "    for i in range(len(data)):\n",
    "        ds=pydicom.dcmread(data[i][0],force=True)\n",
    "        cbct_data.append(ds)\n",
    "\n",
    "    indices = [17000, 17010, 17020, 17030, 17040, 17060, 17070, 17080, 17090, 17100,\n",
    "           18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, \n",
    "           19000, 19050, 19150, 19250, 19350, 19450, 19550, 19650, 19750, 19850,\n",
    "           19950, 20050, 20150, 20250, 20350, 20450, 20550, 20650, 20750, 20850,\n",
    "           20950, 22030, 22130, 22230, 22330, 22430, 22530, 22630, 22730, 22830,\n",
    "           23070, 23170, 23270, 23370, 23470, 23570, 23670, 23770, 23870, 23970]\n",
    "\n",
    "    cbctdata = [cbct_data[i] for i in indices]\n",
    "\n",
    "    \n",
    "\n",
    "    start_time = time.time()\n",
    "    for idx, (a_img, b_img) in enumerate(testloader):\n",
    "\n",
    "        model.eval()\n",
    "        a=a_img.type(torch.cuda.FloatTensor)\n",
    "        b=b_img.type(torch.cuda.FloatTensor)\n",
    "\n",
    "        fake_B = model(a,b).detach()\n",
    "        b = fake_B.cpu().numpy()\n",
    "        c =b.copy()\n",
    "        for s in range(len(a_img)):\n",
    "            t=s+idx*batch_sizes\n",
    "            write_dicom(cbctdata[t],c[s],path,t)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c5cc1-e1e8-47e2-a0c5-4b4d9596b694",
   "metadata": {},
   "source": [
    "##### "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

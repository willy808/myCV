{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "771c74f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "from timm.models.layers import DropPath, to_2tuple, trunc_normal_\n",
    "from timm.models.registry import register_model\n",
    "import math\n",
    "import pydicom\n",
    "import numpy as np\n",
    "import torch, os\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from torch.utils.data import Dataset\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange\n",
    "from PIL import Image\n",
    "import glob\n",
    "from torchsummary import summary\n",
    "from torchvision.transforms import Compose, Resize, ToTensor\n",
    "\n",
    "from einops import rearrange, reduce, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8855145a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    return (x - x.mean()) / (x.max() - x.min())\n",
    "#dataset for cbct dicom\n",
    "def load_dcm(image_path):\n",
    "#     image_data=[]\n",
    "#     for i in range(len(image_path)):\n",
    "    ds=pydicom.dcmread(image_path)\n",
    "    ww=ds.WindowWidth\n",
    "    wc=ds.WindowCenter\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "\n",
    "    ymin = 0\n",
    "    ymax = 255\n",
    "\n",
    "    pixel_array = ds.pixel_array * slope + intercept\n",
    "\n",
    "    # linear exact array\n",
    "    linear_exact_array = np.zeros(pixel_array.shape)\n",
    "\n",
    "    linear_exact_array_less_idx = pixel_array <= (wc - ww/2)\n",
    "    linear_exact_array_large_idx = pixel_array > (wc + ww/2)\n",
    "    linear_exact_array = ((pixel_array - wc)/ww + 0.5) * (ymax - ymin) + ymin\n",
    "    linear_exact_array[linear_exact_array_less_idx] = ymin\n",
    "    linear_exact_array[linear_exact_array_large_idx] = ymax\n",
    "    linear_exact_array = linear_exact_array.astype(np.uint8)        \n",
    "    #add channel\n",
    "\n",
    "    ct=np.expand_dims(linear_exact_array,axis=0)\n",
    "    ct = np.transpose(ct, (2, 1, 0))\n",
    "    ct = np.clip(ct, -200, 2000)\n",
    "    # ct=normalize(ct)        \n",
    "    #because ndarray does not support uint16 , must change to float\n",
    "    ct=ct.astype(float)\n",
    "\n",
    "    return ct #image_data\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Load data from .npy file\n",
    "data = np.load('./result.npy')\n",
    "\n",
    "# Create an empty numpy array to store the loaded data\n",
    "data_load = np.empty((len(data), 2), dtype=object)\n",
    "\n",
    "# Load each pair of data and store it in the numpy array\n",
    "for i, pair in enumerate(data):\n",
    "    data_load[i, 0] = load_dcm(pair[0])\n",
    "    data_load[i, 1] = load_dcm(pair[1])\n",
    "    \n",
    "# print(len(data_load))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82a9760a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "# import cv2\n",
    "import random\n",
    "\n",
    "class ImageDatasetLoader(Dataset):\n",
    "    def __init__(self, mode='train', transformers=None, test_indices=[17000,17010,17020,17030,17040,17060,17070,17080,17090,17100,\n",
    "                                                                      18000,18100,18200,18300,18400,18500,18600,18700,18800,18900, \n",
    "                                                                      19000,19050,19150,19250,19350,19450,19550,19650,19750,19850,\n",
    "                                                                      19950,20050,20150,20250,20350,20450,20550,20650,20750,20850,\n",
    "                                                                      20950,22030,22130,22230,22330,22430,22530,22630,22730,22830,\n",
    "                                                                      23070,23170,23270,23370,23470,23570,23670,23770,23870,23970,\n",
    "                                                                     ]):\n",
    "        self.mode = mode\n",
    "        self.transform = transformers\n",
    "        \n",
    "        # Assuming you have the complete dataset in the 'data_load' variable\n",
    "        total_samples = len(data_load)\n",
    "        \n",
    "        if mode == 'train':\n",
    "            self.files_CBCT = data_load[0:17000, 0]\n",
    "            self.files_sim = data_load[0:17000, 1]\n",
    "        elif mode == 'train1':\n",
    "            self.files_CBCT = data_load[0:17000, 0]\n",
    "            self.files_sim = data_load[0:17000, 1]\n",
    "        elif mode == 'test':\n",
    "            if test_indices is None:\n",
    "                # Randomly select 60 samples for the test set\n",
    "                test_indices = random.sample(range(17000, total_samples), 60)\n",
    "            self.files_CBCT = data_load[test_indices, 0]\n",
    "            self.files_sim = data_load[test_indices, 1]\n",
    "        elif mode == 'fid':\n",
    "            self.files_CBCT = data_load[21704:24704, 0]\n",
    "            self.files_sim = data_load[21704:24704, 1]\n",
    "        elif mode == 'evl':\n",
    "            self.files_CBCT = data_load[24000:24704, 0]\n",
    "            self.files_sim = data_load[24000:24704, 1]\n",
    "        # elif mode == 'evl':\n",
    "        #     if test_indices is None:\n",
    "        #         # Randomly select 60 samples for the test set\n",
    "        #         test_indices = random.sample(range(17000, total_samples), 60)\n",
    "        #     self.files_CBCT = data_load[test_indices, 0]\n",
    "        #     self.files_sim = data_load[test_indices, 1]\n",
    "\n",
    "        self.cbctnorm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.simctnorm = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        self.alltransform = transforms.Compose([\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files_sim)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        file_A = self.files_CBCT[index]\n",
    "        file_B = self.files_sim[index]\n",
    "        if self.transform is not None:\n",
    "            if self.mode == 'train': \n",
    "                img_A = self.alltransform(file_A)\n",
    "                img_B = self.alltransform(file_B)\n",
    "                file_cat = torch.cat((img_A, img_B, img_B), dim=0)\n",
    "                img_cat = self.transform(file_cat)\n",
    "                img_A, img_B, xx = torch.split(img_cat, [1, 1, 1], dim=0)\n",
    "                img_A = self.cbctnorm(file_A)\n",
    "                img_B = self.simctnorm(file_B)\n",
    "            else:\n",
    "                img_A = self.cbctnorm(file_A)\n",
    "                img_B = self.simctnorm(file_B)\n",
    "        return img_A, img_B\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "import torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam,AdamW\n",
    "from tqdm import tqdm\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# Define the custom transform\n",
    "class NumpyToPIL(object):\n",
    "    def __call__(self, img):\n",
    "        return Image.fromarray(np.uint8(img))\n",
    "# Define your transforms\n",
    "data_transform = transforms.Compose([\n",
    "    # transforms.ToPILImage(),\n",
    "    # transforms.CenterCrop(size=(350, 350)),  # crop the center part of the image with size 300x300\n",
    "    # transforms.Resize(size=(384, 384)),  # resize the cropped image to 384x384\n",
    "    transforms.RandomHorizontalFlip(),  # randomly flip the image horizontally with a probability of 0.5\n",
    "    transforms.RandomVerticalFlip(),  # randomly flip the image vertically with a probability of 0.5\n",
    "    transforms.ColorJitter(brightness=0.1, contrast=0.2),  # randomly adjust brightness, contrast, saturation, and hue\n",
    "    transforms.RandomRotation(degrees=15),  # randomly rotate the image by up to 15 degrees\n",
    "    # transforms.RandomCrop(size=(300, /300)),  # randomly crop the image to size 300x300\n",
    "    # transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),  # randomly apply Gaussian blur\n",
    "    # transforms.ToTensor(),  # convert the image to a PyTorch tensor\n",
    "    # transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])\n",
    "transforms_ = transforms.Compose([\n",
    "    # transforms.ToTensor(),\n",
    "    # transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ad3de4-50b9-47db-8ae7-5fb2c471dd12",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1258bc2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as transforms\n",
    "import torch, os, sys\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam,AdamW\n",
    "from tqdm import tqdm\n",
    "transforms_ = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(0.5, 0.5, 0.5)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13e18e70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "import time\n",
    "import argparse\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision.utils import make_grid, save_image\n",
    "\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ab28569b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.SiLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.drop = nn.Dropout(0.2)\n",
    "        self.downsample = None\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.downsample = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "        out = self.drop(out)\n",
    "        return out\n",
    "    \n",
    "#next generation residual block\n",
    "class rSoftMax(nn.Module):\n",
    "    def __init__(self, radix, cardinality):\n",
    "        super().__init__()\n",
    "        assert radix > 0\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch = x.size(0)\n",
    "        if self.radix > 1:\n",
    "            x = x.view(batch, self.cardinality, self.radix, -1).transpose(1, 2)\n",
    "            x = F.softmax(x, dim=1)\n",
    "            x = x.reshape(batch, -1)\n",
    "        else:\n",
    "            x = torch.sigmoid(x)\n",
    "        return x\n",
    "\n",
    "class Splat(nn.Module):\n",
    "    def __init__(self, channels, radix, cardinality, reduction_factor=4):\n",
    "        super(Splat, self).__init__()\n",
    "        self.radix = radix\n",
    "        self.cardinality = cardinality\n",
    "        self.channels = channels\n",
    "        inter_channels = max(channels*radix//reduction_factor, 32)\n",
    "        self.fc1 = nn.Conv2d(channels//radix, inter_channels, 1, groups=cardinality)\n",
    "#         self.bn1 = nn.BatchNorm2d(inter_channels)\n",
    "        self.bn1 = nn.InstanceNorm2d(inter_channels)\n",
    "        self.relu = nn.SiLU(inplace=True)\n",
    "        self.fc2 = nn.Conv2d(inter_channels, channels*radix, 1, groups=cardinality)\n",
    "        self.rsoftmax = rSoftMax(radix, cardinality)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch, rchannel = x.shape[:2]\n",
    "        if self.radix > 1:\n",
    "            splited = torch.split(x, rchannel//self.radix, dim=1)\n",
    "            gap = sum(splited) \n",
    "        else:\n",
    "            gap = x\n",
    "        gap = F.adaptive_avg_pool2d(gap, 1)\n",
    "        gap = self.fc1(gap)\n",
    "\n",
    "        gap = self.bn1(gap)\n",
    "        gap = self.relu(gap)\n",
    "\n",
    "        atten = self.fc2(gap)\n",
    "        atten = self.rsoftmax(atten).view(batch, -1, 1, 1)\n",
    "\n",
    "        if self.radix > 1:\n",
    "            attens = torch.split(atten, rchannel//self.radix, dim=1)\n",
    "            out = sum([att*split for (att, split) in zip(attens, splited)])\n",
    "        else:\n",
    "            out = atten * x\n",
    "        return out.contiguous()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "70c8ae37",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ds_conv2d(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, kernel_size, stride=1, \n",
    "                 dilation=[1,3,5], groups=1, bias=True, \n",
    "                 act_layer='nn.SiLU(True)', init='kaiming'):\n",
    "        super().__init__()\n",
    "        assert in_planes%groups==0\n",
    "        assert kernel_size==3, 'only support kernel size 3 now'\n",
    "        self.in_planes = in_planes\n",
    "        self.out_planes = out_planes\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.dilation = dilation\n",
    "        self.groups = groups\n",
    "        self.with_bias = bias\n",
    "        \n",
    "        self.weight = nn.Parameter(torch.randn(out_planes, in_planes//groups, kernel_size, kernel_size), requires_grad=True)\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_planes))\n",
    "        else:\n",
    "            self.bias = None\n",
    "        self.act = eval(act_layer)\n",
    "        self.init = init\n",
    "        self._initialize_weights()\n",
    "\n",
    "    def _initialize_weights(self):\n",
    "        if self.init == 'dirac':\n",
    "            nn.init.dirac_(self.weight, self.groups)\n",
    "        elif self.init == 'kaiming':\n",
    "            nn.init.kaiming_uniform_(self.weight)\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "        if self.with_bias:\n",
    "            if self.init == 'dirac':\n",
    "                nn.init.constant_(self.bias, 0.)\n",
    "            elif self.init == 'kaiming':\n",
    "                bound = self.groups / (self.kernel_size**2 * self.in_planes)\n",
    "                bound = math.sqrt(bound)\n",
    "                nn.init.uniform_(self.bias, -bound, bound)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "\n",
    "    def forward(self, x):\n",
    "        output = 0\n",
    "        for dil in self.dilation:\n",
    "            output += self.act(\n",
    "                F.conv2d(\n",
    "                    x, weight=self.weight, bias=self.bias, stride=self.stride, padding=dil,\n",
    "                    dilation=dil, groups=self.groups,\n",
    "                )\n",
    "            )\n",
    "        return output\n",
    "class AtrousSelfAttention(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, dilation=1, num_heads=2):\n",
    "        super(AtrousSelfAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = out_channels // num_heads\n",
    "        self.query_conv = ds_conv2d(in_channels, out_channels, kernel_size)\n",
    "#         self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "        self.key_conv = nn.Conv2d(in_channels, out_channels,1)\n",
    "        self.value_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    " \n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_uniform_(self.query_conv.weight)\n",
    "        nn.init.kaiming_uniform_(self.key_conv.weight)\n",
    "        nn.init.kaiming_uniform_(self.value_conv.weight)\n",
    "        nn.init.zeros_(self.query_conv.bias)\n",
    "        nn.init.zeros_(self.key_conv.bias)\n",
    "        nn.init.zeros_(self.value_conv.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size, channels, height, width = x.size()\n",
    "        queries = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "        keys = self.key_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 2, 3)\n",
    "        values = self.value_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "        energy = torch.matmul(queries, keys)\n",
    "        attention = torch.softmax(energy / self.head_dim**0.5, dim=-1)\n",
    "#         attention = self.dropout(attention)\n",
    "        out = torch.matmul(attention, values)\n",
    "        out = out.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, height, width)\n",
    "        return out\n",
    "class Mlp(nn.Module):\n",
    "    def __init__(self, in_features, hidden_features=None,\n",
    "                 out_features=None, act_layer=nn.GELU,\n",
    "                 drop=0.2, with_depconv=True):\n",
    "        super().__init__()\n",
    "        out_features = out_features or in_features\n",
    "        hidden_features = hidden_features or in_features\n",
    "        self.with_depconv = with_depconv\n",
    "        \n",
    "        if self.with_depconv:\n",
    "            self.fc1 = nn.Conv2d(\n",
    "                in_features, hidden_features, 1, stride=1, padding=0, dilation=1, \n",
    "                groups=1, bias=True,\n",
    "            )\n",
    "            self.depconv = nn.Conv2d(\n",
    "                hidden_features, hidden_features, 3, stride=1, padding=1, dilation=1, \n",
    "                groups=hidden_features, bias=True,\n",
    "            )\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Conv2d(\n",
    "                hidden_features, out_features, 1, stride=1, padding=0, dilation=1, \n",
    "                groups=1, bias=True,\n",
    "            )\n",
    "        else:\n",
    "            self.fc1 = nn.Linear(in_features, hidden_features)\n",
    "            self.act = act_layer()\n",
    "            self.fc2 = nn.Linear(hidden_features, out_features)\n",
    "        self.drop = nn.Dropout(drop)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        if self.with_depconv:\n",
    "            x = self.fc1(x)\n",
    "            x = self.depconv(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x\n",
    "        else:\n",
    "            x = self.fc1(x)\n",
    "            x = self.act(x)\n",
    "            x = self.drop(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.drop(x)\n",
    "            return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6fdb7b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "\n",
    "class CSA(nn.Module):\n",
    "    def __init__(self, in_dim, out_dim, num_heads, kernel_size=3, padding=1, stride=2,\n",
    "                 qkv_bias=False, qk_scale=None, attn_drop=0., proj_drop=0.):\n",
    "        super().__init__()\n",
    "        self.in_dim = in_dim\n",
    "        self.out_dim = out_dim\n",
    "        head_dim = out_dim // num_heads\n",
    "        self.num_heads = num_heads\n",
    "        self.kernel_size = kernel_size\n",
    "        self.padding = padding\n",
    "        self.stride = stride\n",
    "        self.scale = qk_scale or head_dim**-0.5\n",
    "        \n",
    "        self.attn = nn.Linear(in_dim, kernel_size**4 * num_heads)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "\n",
    "        self.unfold = nn.Unfold(kernel_size=kernel_size, padding=padding, stride=stride)\n",
    "        self.pool = nn.AvgPool2d(kernel_size=stride, stride=stride, ceil_mode=True)\n",
    "        \n",
    "        self.csa_group = 1\n",
    "        assert out_dim % self.csa_group == 0\n",
    "        self.weight = nn.Conv2d(\n",
    "            self.kernel_size*self.kernel_size*out_dim, \n",
    "            self.kernel_size*self.kernel_size*out_dim, \n",
    "            1, \n",
    "            stride=1, padding=0, dilation=1, \n",
    "            groups=self.kernel_size*self.kernel_size*self.csa_group, \n",
    "            bias=qkv_bias,\n",
    "        )\n",
    "        assert qkv_bias == False\n",
    "        fan_out = self.kernel_size*self.kernel_size*self.out_dim\n",
    "        fan_out //= self.csa_group\n",
    "        self.weight.weight.data.normal_(0, math.sqrt(2.0 / fan_out)) # init\n",
    "        \n",
    "        self.proj = nn.Linear(out_dim, out_dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        \n",
    "    def forward(self, x, v=None):\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        B, H, W, _ = x.shape\n",
    "        h, w = math.ceil(H / self.stride), math.ceil(W / self.stride)\n",
    "        \n",
    "        attn = self.pool(x.permute(0, 3, 1, 2)).permute(0, 2, 3, 1)\n",
    "        attn = self.attn(attn).reshape(\n",
    "            B, h * w, self.num_heads, self.kernel_size * self.kernel_size,\n",
    "            self.kernel_size * self.kernel_size).permute(0, 2, 1, 3, 4) # B,H,N,kxk,kxk\n",
    "        attn = attn * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "        \n",
    "        v = x.permute(0, 3, 1, 2) # B,C,H, W\n",
    "        v = self.unfold(v).reshape(\n",
    "            B, self.out_dim, self.kernel_size*self.kernel_size, h*w\n",
    "        ).permute(0,3,2,1).reshape(B*h*w, self.kernel_size*self.kernel_size*self.out_dim, 1, 1)\n",
    "        v = self.weight(v)\n",
    "        v = v.reshape(B, h*w, self.kernel_size*self.kernel_size, self.num_heads, \n",
    "                      self.out_dim//self.num_heads).permute(0,3,1,2,4).contiguous() # B,H,N,kxk,C/H\n",
    "        \n",
    "        x = (attn @ v).permute(0, 1, 4, 3, 2)\n",
    "        x = x.reshape(B, self.out_dim * self.kernel_size * self.kernel_size, h * w)\n",
    "        x = F.fold(x, output_size=(H, W), kernel_size=self.kernel_size, \n",
    "                   padding=self.padding, stride=self.stride)\n",
    "\n",
    "        x = self.proj(x.permute(0, 2, 3, 1))\n",
    "        x = self.proj_drop(x)\n",
    "        return x.permute(0, 3, 1, 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fef769c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "# class cross_attention(nn.Module):\n",
    "#     def __init__(self, in_channels,num_heads=2, kernel_size=3, dilation=1):\n",
    "#         super(cross_attention, self).__init__()\n",
    "#         out_channels = in_channels\n",
    "#         self.num_heads = num_heads\n",
    "#         self.head_dim = out_channels // num_heads\n",
    "#         self.query_conv = nn.Conv2d(in_channels, out_channels,3,1,1)\n",
    "# #         self.key_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1)\n",
    "#         self.key_conv = nn.Conv2d(in_channels, out_channels,1)\n",
    "#         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "# #         self.value_conv = nn.Conv2d(in_channels, out_channels, 1)\n",
    "\n",
    "#         self.reset_parameters()\n",
    "\n",
    "#     def reset_parameters(self):\n",
    "#         nn.init.kaiming_uniform_(self.query_conv.weight)\n",
    "#         nn.init.kaiming_uniform_(self.key_conv.weight)\n",
    "#         nn.init.kaiming_uniform_(self.value_conv.weight)\n",
    "#         nn.init.zeros_(self.query_conv.bias)\n",
    "#         nn.init.zeros_(self.key_conv.bias)\n",
    "#         nn.init.zeros_(self.value_conv.bias)\n",
    "\n",
    "#     def forward(self, x, k, v):\n",
    "#         batch_size, channels, height, width = x.size()\n",
    "#         queries = self.query_conv(x).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 3, 2)\n",
    "#         keys = self.key_conv(k).view(batch_size, self.num_heads, self.head_dim, height*width).permute(0, 1, 2, 3)\n",
    "#         values = self.value_conv(v).view(batch_size, self.num_heads, height*width, self.head_dim)\n",
    "#         energy = torch.matmul(queries, keys)\n",
    "#         attention = torch.softmax(energy / self.head_dim**0.5, dim=-1)\n",
    "#         out = torch.matmul(attention, values)\n",
    "#         out = out.permute(0, 1, 3, 2).contiguous().view(batch_size, -1, height, width)\n",
    "#         return out\n",
    "class cross_attention(nn.Module):\n",
    "    def __init__(self,in_channels) :\n",
    "        super(cross_attention, self).__init__()\n",
    "        self.queryConv =nn.Sequential(nn.Conv2d(in_channels, in_channels,1,1), \n",
    "                                      nn.GroupNorm(in_channels//4,in_channels)) \n",
    "        self.keyConv = nn.Sequential(nn.Conv2d(in_channels,in_channels,1,1), \n",
    "                                     nn.GroupNorm(in_channels//4,in_channels)) \n",
    "        self.psiConv = nn.Sequential(\n",
    "            nn.GroupNorm(in_channels//4,in_channels),\n",
    "            nn.Conv2d(in_channels,1,1,1), \n",
    "            nn.SiLU()) \n",
    "    def forward(self, query, key):\n",
    "        value = key\n",
    "        key = self.keyConv(key)\n",
    "        query = self.queryConv(query)\n",
    "        psi = F.silu(key+query)\n",
    "        psi = self.psiConv(psi)\n",
    "        return value*psi\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cfe21ad7-34ca-423e-9748-72d683b09348",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SPADE(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=1):\n",
    "        super(SPADE, self).__init__()\n",
    "        self.norm = nn.GroupNorm(in_channels//4,in_channels)\n",
    "        self.share = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "        self.conv_gamma = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "        self.conv_beta = nn.Conv2d(in_channels, in_channels, kernel_size, stride, padding)\n",
    "\n",
    "    def forward(self, x, segmap):\n",
    "        normalized = self.norm(x)\n",
    "        seg_share  = self.share(F.silu(self.norm(segmap)))\n",
    "        gamma = self.conv_gamma(F.silu(self.norm(seg_share)))\n",
    "        #beta = F.silu(self.conv_beta(self.norm(x)))\n",
    "        return (1 + gamma) * normalized #+ beta\n",
    "\n",
    "class style_encoder(nn.Module):\n",
    "    def __init__(self,in_channels,embed_dim ):\n",
    "        super(style_encoder,self).__init__()\n",
    "        self.embed_dim =embed_dim \n",
    "        self.styleconv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, self.embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleconv2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "            \n",
    "        self.styleconv3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleconv4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*4),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.upUpsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        self.styleup4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*6,self.embed_dim*2, 3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//2,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleup3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*3,self.embed_dim*1, 3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim*1),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        self.styleup2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2,self.embed_dim*1,3,1,1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim*1),\n",
    "            nn.SiLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "    def forward (self,x):\n",
    "        x1 = self.styleconv1(x)\n",
    "        x2 = self.styleconv2(x1)\n",
    "        x3 = self.styleconv3(x2)\n",
    "        x4 = self.styleconv4(x3)\n",
    "        \n",
    "        x5 = self.upUpsample(x4)\n",
    "        x5 = torch.cat((x5,x3),dim=1)\n",
    "        x5 = self.styleup4(x5)\n",
    "        \n",
    "        x6 = self.upUpsample(x5)\n",
    "        x6 = torch.cat((x6,x2),dim=1)\n",
    "        x6 = self.styleup3(x6)\n",
    "        \n",
    "        x7 = self.upUpsample(x6)\n",
    "        x7 = torch.cat((x7,x1),dim=1)\n",
    "        x7 = self.styleup2(x7)\n",
    "        \n",
    "        return x7, x6, x5, x4\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b2cc5adc-5b2e-4380-9f2f-0988e2914e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, kernel_size, stride, bias=True, norm=False, relu=True, transpose=False):\n",
    "        super(BasicConv, self).__init__()\n",
    "        if bias and norm:\n",
    "            bias = False\n",
    "\n",
    "        padding = kernel_size // 2\n",
    "        layers = list()\n",
    "        if transpose:\n",
    "            padding = kernel_size // 2 -1\n",
    "            layers.append(nn.ConvTranspose2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        else:\n",
    "            layers.append(\n",
    "                nn.Conv2d(in_channel, out_channel, kernel_size, padding=padding, stride=stride, bias=bias))\n",
    "        if norm:\n",
    "            layers.append(nn.BatchNorm2d(out_channel))\n",
    "        if relu:\n",
    "            layers.append(nn.ReLU(inplace=True))\n",
    "        self.main = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x)\n",
    "\n",
    "\n",
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel, norm=False):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            BasicConv(in_channel, out_channel, kernel_size=3, stride=1, norm=norm, relu=True),\n",
    "            BasicConv(out_channel, out_channel, kernel_size=3, stride=1, norm=norm, relu=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.main(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d37c1140",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGenerator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.overlappatch_size=48\n",
    "        self.patch_size = 96\n",
    "        self.num_patch=(384//self.patch_size)\n",
    "        self.num_patches=self.num_patch**2\n",
    "        self.embed_dim = 64\n",
    "        \n",
    "        \n",
    "        self.patches = nn.Sequential(\n",
    "            nn.Conv2d(1, self.embed_dim,  kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        self.down1 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim,  kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(self.embed_dim//4,self.embed_dim),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # self.cross_attention1 = cross_attention(in_channels=self.embed_dim, num_heads=2)\n",
    "        self.cross_attention1 = cross_attention(in_channels=self.embed_dim)\n",
    "        self.attn1 = CSA(in_dim=self.embed_dim, out_dim=self.embed_dim, num_heads=2)\n",
    "        self.pos_embed1 = nn.Parameter(torch.zeros(1, self.embed_dim, self.patch_size,self.patch_size))\n",
    "        self.norm1 = nn.GroupNorm(self.embed_dim//4,self.embed_dim)\n",
    "        self.mlp1 = Mlp(self.embed_dim,self.embed_dim*4)\n",
    "        # self.ResidualBlock1 = ResidualBlock(self.embed_dim,self.embed_dim)\n",
    "        \n",
    "        self.down2 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim, self.embed_dim*2,  kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention2 = cross_attention(in_channels=self.embed_dim*2, num_heads=2)\n",
    "        self.cross_attention2 = cross_attention(in_channels=self.embed_dim*2)\n",
    "        self.attn2 = AtrousSelfAttention(self.embed_dim*2, self.embed_dim*2, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed2 = nn.Parameter(torch.zeros(1, self.embed_dim*2, self.patch_size//2, self.patch_size//2))\n",
    "        self.norm2 =nn.GroupNorm(32,self.embed_dim*2)\n",
    "        self.mlp2 = Mlp(self.embed_dim*2,self.embed_dim*2*8)\n",
    "        # self.ResidualBlock2 = ResidualBlock(self.embed_dim*2,self.embed_dim*2)\n",
    "\n",
    "        \n",
    "        self.down3 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,self.embed_dim*4),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention3 = cross_attention(in_channels=self.embed_dim*4, num_heads=2)\n",
    "        self.cross_attention3 = cross_attention(in_channels=self.embed_dim*4)\n",
    "        self.attn3 = AtrousSelfAttention(self.embed_dim*4, self.embed_dim*4, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed3 = nn.Parameter(torch.zeros(1, self.embed_dim*4, self.patch_size//4, self.patch_size//4))\n",
    "        self.norm3 = nn.GroupNorm(32,self.embed_dim*4)\n",
    "        self.mlp3 = Mlp(self.embed_dim*4,self.embed_dim*4*4)\n",
    "        # self.ResidualBlock3 = ResidualBlock(self.embed_dim*4,self.embed_dim*4)\n",
    "\n",
    "        \n",
    "        self.down4 = nn.Sequential(\n",
    "            nn.Conv2d(self.embed_dim*4, self.embed_dim*8,  kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        # self.cross_attention4 = cross_attention(in_channels=self.embed_dim*1, num_heads=2)\n",
    "        self.cross_attention4 = cross_attention(in_channels=self.embed_dim*1)\n",
    "        self.attn4 = AtrousSelfAttention(self.embed_dim*8, self.embed_dim*8, kernel_size=3, dilation=1, num_heads=4)\n",
    "        self.pos_embed4 = nn.Parameter(torch.zeros(1, self.embed_dim*8, self.patch_size//8, self.patch_size//8))\n",
    "        self.norm4 = nn.GroupNorm(self.embed_dim//2,self.embed_dim*8)\n",
    "        self.mlp4 = Mlp(self.embed_dim*8,self.embed_dim*8*4)\n",
    "        # self.ResidualBlock4 = ResidualBlock(self.embed_dim*8,self.embed_dim*8)\n",
    "#         self.ResidualBlock4 = Splat(self.embed_dim*8,2,8)\n",
    "\n",
    "        self.upUpsample = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # self.threethree = SPADE(self.embed_dim*4, self.embed_dim*4)\n",
    "        self.con3 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=3, padding=1, stride=1),\n",
    "        )\n",
    "        self.con33 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*8),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=1),\n",
    "            # nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=1),\n",
    "             nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.con33 = nn.Sequential(\n",
    "        #     nn.GroupNorm(32,self.embed_dim*8),\n",
    "        #     nn.SiLU(True),\n",
    "        #     nn.ConvTranspose2d(self.embed_dim*8, self.embed_dim*4, kernel_size=4, padding=1, stride=2, bias=False),\n",
    "        #     # nn.Conv2d(self.embed_dim*8,self.embed_dim*4, kernel_size=1),\n",
    "        #      nn.Dropout(0.2)\n",
    "        # )\n",
    "            \n",
    "        self.con2 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*4),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*4,self.embed_dim*2, kernel_size=3, padding=1, stride=1),\n",
    "            \n",
    "        )\n",
    "        self.con22 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*12),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*12, self.embed_dim*2, kernel_size=3, padding=1, stride=1, bias=True),\n",
    "            nn.Dropout(0.2)\n",
    "            \n",
    "        )\n",
    "#         self.con22 = nn.Sequential(\n",
    "#             nn.GroupNorm(32,self.embed_dim*12),\n",
    "#             nn.SiLU(True),\n",
    "#             nn.ConvTranspose2d(self.embed_dim*12, self.embed_dim*2, kernel_size=4, padding=1, stride=2, bias=False),\n",
    "#             nn.Dropout(0.2)\n",
    "            \n",
    "#         )\n",
    "\n",
    "        \n",
    "        #48->96\n",
    "\n",
    "        self.oneone =SPADE(self.embed_dim*1, self.embed_dim*1)\n",
    "        self.con11 = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*10),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*10, self.embed_dim*1,kernel_size=3, padding=1, stride=1, bias=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        # self.con11 = nn.Sequential(\n",
    "        #     nn.GroupNorm(32,self.embed_dim*10),\n",
    "        #     nn.SiLU(True),\n",
    "        #     nn.ConvTranspose2d(self.embed_dim*10, self.embed_dim*1, kernel_size=4, padding=1, stride=2, bias=False),\n",
    "        #     nn.Dropout(0.2)\n",
    "        # )\n",
    "        self.con1 = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*2,self.embed_dim*1, kernel_size=1),\n",
    "        )\n",
    "        # self.con11 = nn.Conv2d(self.embed_dim*2,self.embed_dim*1, kernel_size=3, stride=1, padding=1)\n",
    "        \n",
    "\n",
    "        self.upup =  SPADE(self.embed_dim*1, self.embed_dim*1)\n",
    "        self.conupup =  nn.Conv2d(self.embed_dim*1,self.embed_dim*1, kernel_size=3, stride=1, padding=1)\n",
    "        self.conupupnorm = nn.GroupNorm(self.embed_dim//4,self.embed_dim)\n",
    "        \n",
    "        # self.transup = nn.Sequential(\n",
    "        #     nn.GroupNorm(32,self.embed_dim*5),\n",
    "        #     nn.SiLU(True),\n",
    "        #     nn.ConvTranspose2d(self.embed_dim*5, self.embed_dim*5, kernel_size=4, padding=1, stride=2, bias=False),\n",
    "        #     nn.Dropout(0.2)\n",
    "        # )\n",
    "        self.transup = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*5),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*5, self.embed_dim*5,  kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.conup = nn.Sequential(\n",
    "            nn.GroupNorm(32,self.embed_dim*6),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*6,self.embed_dim*1, kernel_size=1, bias=True),\n",
    "        )\n",
    "        self.transout = nn.Sequential(\n",
    "            nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "            nn.GroupNorm(32,self.embed_dim*2),\n",
    "            nn.SiLU(True),\n",
    "            nn.Conv2d(self.embed_dim*2, self.embed_dim*1,kernel_size=3, stride=1, padding=1, bias=True),\n",
    "            nn.Dropout(0.2)\n",
    "        )\n",
    "        \n",
    "        self.output = nn.Sequential(\n",
    "            nn.SiLU(inplace=True),\n",
    "            nn.Conv2d(self.embed_dim*1, 1, 3,1,1, bias=True),\n",
    "            # nn.Tanh(),\n",
    "        )   \n",
    "        \n",
    "        self.gelu=nn.GELU()\n",
    "        drop_path=0.2\n",
    "        self.drop_path = DropPath(\n",
    "            drop_path) if drop_path > 0. else nn.Identity()\n",
    "        self.transformer_layer = 2\n",
    "        self.style = style_encoder(1,self.embed_dim)\n",
    "\n",
    "        \n",
    "    def forward(self,x,style):\n",
    "        \n",
    "        style1, style2, style3, style4 = self.style(style)\n",
    "        \n",
    "        # x_deblur = self.deblur(x)\n",
    "        #input 1*384*384-> 16*96*96\n",
    "        x1 = self.patches(x)\n",
    "        x2t = self.down1(x1)\n",
    "        #x2r = self.ResidualBlock1(x2t)\n",
    "        x2 = x2t + self.pos_embed1\n",
    "        for i in range(self.transformer_layer):\n",
    "            x2 = x2 + self.drop_path(self.attn1(self.norm1(x2)))\n",
    "            x2 = F.silu(x2)\n",
    "        x2 = x2 + self.drop_path(self.mlp1(self.norm1(x2)))\n",
    "        \n",
    "        #16*96*96 -> 64*48*48\n",
    "        x3t = self.down2(x2)\n",
    "        #x3r = self.ResidualBlock2(x3t)\n",
    "        x3 = x3t + self.pos_embed2\n",
    "        for i in range(self.transformer_layer):\n",
    "            x3 = x3 + self.drop_path(self.attn2(self.norm2(x3)))\n",
    "            x3 = F.silu(x3)\n",
    "        x3 = x3 + self.drop_path(self.mlp2(self.norm2(x3)))\n",
    "\n",
    "        \n",
    "        #64*48*48 ->256*24*24\n",
    "        x4t = self.down3(x3)\n",
    "        #x4r = self.ResidualBlock3(x4t)\n",
    "        x4 = x4t + self.pos_embed3\n",
    "        for i in range(self.transformer_layer):\n",
    "            x4 = x4 + self.drop_path(self.attn3(self.norm3(x4)))\n",
    "            x4 = F.silu(x4)\n",
    "        x4 = x4 + self.drop_path(self.mlp3(self.norm3(x4)))\n",
    "\n",
    "        x5t = self.down4(x4)\n",
    "        #x5r = self.ResidualBlock4(x5t)\n",
    "        x5 = x5t + self.pos_embed4\n",
    "        for i in range(self.transformer_layer):\n",
    "            x5 = x5 + self.drop_path(self.attn4(self.norm4(x5)))\n",
    "            x5 = F.silu(x5)\n",
    "        x5 = x5 + self.drop_path(self.mlp4(self.norm4(x5)))\n",
    "        \n",
    "        # x6 = self.upUpsample(x5)\n",
    "        x6 = self.con33(x5)\n",
    "        # x4_cross = self.cross_attention3(x6,x4,x4)\n",
    "        x4_cross = self.cross_attention3(style4,x6)\n",
    "        x6_cat = torch.cat((x6,x4_cross),dim=1)\n",
    "        x6_cat1 = self.con3(x6_cat)\n",
    "        # x6_cat_cross = self.cross_attention3(x6_cat1,style4)\n",
    "        x6_cat_cross = self.cross_attention3(x6_cat1,x4)\n",
    "        x6_cat = torch.cat((x6_cat1,x6_cat1,x6_cat_cross),dim=1)\n",
    "        # x6_cat = x6_cat1 +  F.silu(self.norm3(self.con33(x6_cat)))\n",
    "        \n",
    "        # x7 = self.upUpsample(x6_cat)\n",
    "        x7 = self.con22(x6_cat)\n",
    "        x3_cross = self.cross_attention2(style3,x7)\n",
    "        x7_cat = torch.cat((x7,x3_cross),dim=1)\n",
    "        x7_cat1 = self.con2(x7_cat)\n",
    "        # x7_cat_cross = self.cross_attention2(x7_cat1,style3)\n",
    "        x7_cat_cross = self.cross_attention2(x7_cat1,x3)\n",
    "        x7_cat = torch.cat((x7_cat1,x7_cat1,x7_cat1,x7_cat_cross,x7_cat_cross),dim=1)\n",
    "        # x7_cat = x7_cat1 + F.silu(self.norm2(self.con22(x7_cat)))\n",
    "        \n",
    "        # x8 = self.upUpsample(x7_cat)\n",
    "        x8 = self.con11(x7_cat)\n",
    "        # x2_cross = self.cross_attention1(x8,x2,x2)\n",
    "        x2_cross = self.cross_attention1(x8,style2)\n",
    "        x8_cat = torch.cat((x8,x2_cross),dim=1)\n",
    "        x8_cat1 = self.con1(x8_cat)\n",
    "\n",
    "        # x8_cat_cross = self.cross_attention1(x8_cat1,style2)\n",
    "        x8_cat_cross = self.cross_attention1(x8_cat1,x2)\n",
    "        x8_cat = torch.cat((x8_cat1,x8_cat1,x8_cat_cross,x8_cat_cross,x8_cat_cross),dim=1)\n",
    "        # x8_cat = x8_cat1 + F.silu(self.norm1(self.con11(x8_cat)))\n",
    "        \n",
    "        x9 = self.transup(x8_cat)\n",
    "        x9_cat = torch.cat((x9,style1),dim=1)\n",
    "        x9_cat = self.conup(x9_cat)\n",
    "        # x9_cat_cross = self.cross_attention4(x9_cat,style1)\n",
    "        x9_cat_cross = self.cross_attention4(x9_cat,x1)\n",
    "\n",
    "        x9_cat1 = torch.cat((x9_cat,x9_cat_cross),dim=1)\n",
    "        \n",
    "        x11 = self.transout(x9_cat1)\n",
    "        x11 = self.output(x11)\n",
    "        return x11\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2c8b5dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FPSE discriminator \n",
    "class TDiscriminator(nn.Module):\n",
    "    def __init__(self, input_nc=1, nf=64, label_nc=1):\n",
    "        super(TDiscriminator, self).__init__()\n",
    "        # bottom-up pathway\n",
    "        \n",
    "        self.enc1 = nn.Sequential(\n",
    "            nn.Conv2d(input_nc, nf, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(nf//4,nf),\n",
    "            nn.SiLU(True))\n",
    "        self.enc2 = nn.Sequential(\n",
    "            nn.Conv2d(nf, nf*2, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.enc3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*2, nf*4, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.enc4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*8),\n",
    "            nn.SiLU(True))\n",
    "        self.enc5 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*8, kernel_size=3, stride=2, padding=1),\n",
    "            nn.GroupNorm(32,nf*8),\n",
    "            nn.SiLU(True))\n",
    "\n",
    "        # top-down pathway\n",
    "        self.lat2 = nn.Sequential(\n",
    "            nn.Conv2d(nf*2, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        self.lat5 = nn.Sequential(\n",
    "            nn.Conv2d(nf*8, nf*4, kernel_size=1),\n",
    "            nn.GroupNorm(32,nf*4),\n",
    "            nn.SiLU(True))\n",
    "        \n",
    "        # upsampling\n",
    "        self.up = nn.Upsample(scale_factor=2, mode='nearest')\n",
    "        \n",
    "        # final layers\n",
    "        self.final2 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.final3 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "        self.final4 = nn.Sequential(\n",
    "            nn.Conv2d(nf*4, nf*2, kernel_size=3, stride=1, padding=1),\n",
    "            nn.GroupNorm(nf//2,nf*2),\n",
    "            nn.SiLU(True))\n",
    "    \n",
    "        # true/false prediction and semantic alignment prediction\n",
    "        self.tf = nn.Conv2d(nf*2, 1, kernel_size=1)\n",
    "        self.seg = nn.Conv2d(nf*2, nf*2, kernel_size=1)\n",
    "        self.embedding = nn.Conv2d(label_nc, nf*2, kernel_size=1)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "\n",
    "    def forward(self, fake_and_real_img, segmap):\n",
    "        # bottom-up pathway\n",
    "        feat11 = self.enc1(fake_and_real_img)  #384->192\n",
    "        feat12 = self.enc2(feat11)   #192->96\n",
    "        feat13 = self.enc3(feat12)   #96->48\n",
    "        feat14 = self.enc4(feat13)  #48->24\n",
    "        feat15 = self.enc5(feat14)   #24->12\n",
    "        # top-down pathway and lateral connections\n",
    "        feat25 = self.lat5(feat15)\n",
    "        feat24 = self.up(feat25) + self.lat4(feat14)  #24\n",
    "        feat23 = self.up(feat24) + self.lat3(feat13)  #48\n",
    "        feat22 = self.up(feat23) + self.lat2(feat12)  #96\n",
    "        # final prediction layers\n",
    "        feat32 = self.final2(feat22)\n",
    "        feat33 = self.final3(feat23)\n",
    "        feat34 = self.final4(feat24)\n",
    "        # Patch-based True/False prediction\n",
    "        pred2 = self.tf(feat32)\n",
    "        pred3 = self.tf(feat33)\n",
    "        pred4 = self.tf(feat34)\n",
    "        seg2 = self.seg(feat32)\n",
    "        seg3 = self.seg(feat33)\n",
    "        seg4 = self.seg(feat34)\n",
    "\n",
    "        # intermediate features for discriminator feature matching loss\n",
    "        feats = [feat12, feat13, feat14, feat15]\n",
    "\n",
    "        # segmentation map embedding\n",
    "        segemb = self.embedding(segmap)\n",
    "        segemb = F.avg_pool2d(segemb, kernel_size=2, stride=2)\n",
    "        segemb2 = F.avg_pool2d(segemb, kernel_size=2, stride=2)\n",
    "        segemb3 = F.avg_pool2d(segemb2, kernel_size=2, stride=2)\n",
    "        segemb4 = F.avg_pool2d(segemb3, kernel_size=2, stride=2)\n",
    "\n",
    "        # semantics embedding discriminator score\n",
    "        pred2 += torch.mul(segemb2, seg2).sum(dim=1, keepdim=True)\n",
    "        pred3 += torch.mul(segemb3, seg3).sum(dim=1, keepdim=True)\n",
    "        pred4 += torch.mul(segemb4, seg4).sum(dim=1, keepdim=True)\n",
    "\n",
    "        # concat results from multiple resolutions\n",
    "        results = [pred2, pred3, pred4]\n",
    "            \n",
    "\n",
    "        return [feats, results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9e71a124",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradientVariance(nn.Module):\n",
    "\n",
    "    def __init__(self, patch_size):\n",
    "        super(GradientVariance, self).__init__()\n",
    "        self.patch_size = patch_size\n",
    "        # Sobel kernel for the gradient map calculation\n",
    "        self.kernel_x = torch.FloatTensor([[-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]).unsqueeze(0).unsqueeze(0).cuda()\n",
    "        self.kernel_y = torch.FloatTensor([[1, 2, 1], [0, 0, 0], [-1, -2, -1]]).unsqueeze(0).unsqueeze(0).cuda()\n",
    "        # operation for unfolding image into non overlapping patches\n",
    "        self.unfold = torch.nn.Unfold(kernel_size=(self.patch_size, self.patch_size), stride=self.patch_size)\n",
    "\n",
    "    def forward(self, output, target):\n",
    "        # calculation of the gradient maps of x and y directions\n",
    "        gx_target = F.conv2d(target, self.kernel_x, stride=1, padding=1)\n",
    "        gy_target = F.conv2d(target, self.kernel_y, stride=1, padding=1)\n",
    "        gx_output = F.conv2d(output, self.kernel_x, stride=1, padding=1)\n",
    "        gy_output = F.conv2d(output, self.kernel_y, stride=1, padding=1)\n",
    "\n",
    "        # unfolding image to patches\n",
    "        gx_target_patches = self.unfold(gx_target)\n",
    "        gy_target_patches = self.unfold(gy_target)\n",
    "        gx_output_patches = self.unfold(gx_output)\n",
    "        gy_output_patches = self.unfold(gy_output)\n",
    "\n",
    "        # calculation of variance of each patch\n",
    "        var_target_x = torch.var(gx_target_patches, dim=1)\n",
    "        var_output_x = torch.var(gx_output_patches, dim=1)\n",
    "        var_target_y = torch.var(gy_target_patches, dim=1)\n",
    "        var_output_y = torch.var(gy_output_patches, dim=1)\n",
    "\n",
    "        # loss function as a MSE between variances of patches extracted from gradient maps\n",
    "        gradvar_loss = F.mse_loss(var_target_x, var_output_x) + F.mse_loss(var_target_y, var_output_y)\n",
    "\n",
    "        return gradvar_loss\n",
    "\n",
    "from torchvision.utils import make_grid\n",
    "from torchmetrics import PeakSignalNoiseRatio,StructuralSimilarityIndexMeasure,MetricCollection\n",
    "Tensor = torch.cuda.FloatTensor if torch.cuda else torch.Tensor\n",
    "def sample_images(real_A, real_B,G_AB,G_BA,epoch,figside=1.5,):\n",
    "    assert real_A.size() == real_B.size(), 'The image size for two domains must be the same'\n",
    "    \n",
    "    G_AB.eval()\n",
    "    G_BA.eval()\n",
    "    \n",
    "    real_A = real_A.type(Tensor)\n",
    "    fake_B = G_AB(real_A).detach()\n",
    "    real_B = real_B.type(Tensor)\n",
    "    fake_A = G_BA(real_B).detach()\n",
    "    psnrA = PeakSignalNoiseRatio(data_range=torch.max(real_A)-torch.min(real_A)).cuda()\n",
    "    psnrB = PeakSignalNoiseRatio(data_range=torch.max(real_B)-torch.min(real_B)).cuda()\n",
    "    ssim = StructuralSimilarityIndexMeasure().cuda()\n",
    "\n",
    "\n",
    "    ans_ssim=ssim(fake_B,real_A)\n",
    "    ans_psnr_toB=psnrB(fake_B,real_B)\n",
    "    ans_psnr_toA=psnrA(fake_B,real_A)\n",
    "    \n",
    "    nrows = real_A.size(0)\n",
    "    real_A = make_grid(real_A, nrow=nrows, normalize=True)\n",
    "    fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n",
    "    real_B = make_grid(real_B, nrow=nrows, normalize=True)\n",
    "    fake_A = make_grid(fake_A, nrow=nrows, normalize=True)\n",
    "    \n",
    "    image_grid = torch.cat((real_A, fake_B, real_B, fake_A), 1).cpu().permute(1, 2, 0)\n",
    "    \n",
    "    plt.figure(figsize=(figside*nrows, figside*4))\n",
    "    plt.imshow(image_grid)\n",
    "    plt.axis('off')\n",
    "    plt.savefig(f'epoch{epoch}plot.png')\n",
    "    plt.show()\n",
    "    plt.close(image_grid)\n",
    "    return ans_ssim,ans_psnr_toB,ans_psnr_toA\n",
    "\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "class MS_SSIM_Loss2(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1*( 1 - super(MS_SSIM_Loss2, self).forward(img1, img2) )\n",
    "MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "class CustomSSIMLoss(SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 * (1 - super(CustomSSIMLoss, self).forward(img1, img2))\n",
    "\n",
    "SSIM_Loss = CustomSSIMLoss(data_range=1.0, size_average=True, channel=1)\n",
    "def unnormalize(sample_dicom, x):\n",
    "    ds = sample_dicom.copy()\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "    ymin = -200\n",
    "    ymax = 1600\n",
    "    pixel_min = ymin #/ slope + intercept\n",
    "    pixel_max = ymax #/ slope + intercept\n",
    "#     normalized_array = np.clip((x + 1) , 0, 2)\n",
    "#     mean = np.mean(x)\n",
    "#     std = np.std(x)\n",
    "#     unnormalized_array = (x * std + mean) * slope + intercept\n",
    "    scaled_array = (x)*6.5-600#(x*0.5+0.5)*100#-500\n",
    "    unnormalized_array = (scaled_array - intercept) / slope \n",
    "    return unnormalized_array\n",
    "\n",
    "\n",
    "def write_dicom(sample_dicom, array, paths, tt):\n",
    "    ds = sample_dicom.copy()\n",
    "    ww = ds.WindowWidth\n",
    "    wc = ds.WindowCenter\n",
    "    array = np.transpose(array, (2, 1, 0))[:, :, 0]\n",
    "    array = unnormalize(sample_dicom, array)\n",
    "    array = np.clip(array, 0, 2 ** 16 - 1)\n",
    "    ds.PixelData = array.astype(np.uint16).tobytes()\n",
    "    return ds.save_as(paths + \"/\" + str(tt) + \".dcm\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ba1972e9-1e48-4c8d-9cdf-9d8ccbed9353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 0 NVIDIA A100-SXM4-80GB\n"
     ]
    }
   ],
   "source": [
    "# all loss function here\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from pytorch_msssim import ssim, ms_ssim, SSIM, MS_SSIM\n",
    "class MS_SSIM_Loss2(MS_SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1*( 1 - super(MS_SSIM_Loss2, self).forward(img1, img2) )\n",
    "MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "class CustomSSIMLoss(SSIM):\n",
    "    def forward(self, img1, img2):\n",
    "        return 1 * (1 - super(CustomSSIMLoss, self).forward(img1, img2))\n",
    "\n",
    "SSIM_Loss = CustomSSIMLoss(data_range=1.0, size_average=True, channel=1)\n",
    "# import torch.autograd as autograd\n",
    "class GDL(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(GDL, self).__init__()\n",
    "\n",
    "    def forward(self, pred, target):\n",
    "        pred_dy = torch.abs(pred[:, :, 1:, :] - pred[:, :, :-1, :])\n",
    "        pred_dx = torch.abs(pred[:, :, :, 1:] - pred[:, :, :, :-1])\n",
    "        target_dy = torch.abs(target[:, :, 1:, :] - target[:, :, :-1, :])\n",
    "        target_dx = torch.abs(target[:, :, :, 1:] - target[:, :, :, :-1])\n",
    "        grad_diff_x = torch.abs(target_dx - pred_dx)\n",
    "        grad_diff_y = torch.abs(target_dy - pred_dy)\n",
    "        return grad_diff_x.mean() + grad_diff_y.mean()\n",
    "\n",
    "class HingeLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(HingeLoss, self).__init__()\n",
    "\n",
    "    def forward(self, predictions,seg,real=True):\n",
    "        # Compute the hinge loss using Swish activation\n",
    "        loss = 0\n",
    "        if real ==True:\n",
    "            for pred in seg:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 - pred))\n",
    "            for preds in predictions:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 - preds))\n",
    "        else:\n",
    "            for pred in seg:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 + pred))\n",
    "            for preds in predictions:\n",
    "                loss += torch.mean(nn.ReLU(inplace=True)(1 + preds))\n",
    "            \n",
    "        return loss\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "torch.cuda.set_device(0)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "import torchvision\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "class Vgg19_out(nn.Module):\n",
    "    def __init__(self, requires_grad=False):\n",
    "        super(Vgg19_out, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        vgg = torchvision.models.vgg19(pretrained=True).to(device) #.cuda()\n",
    "        vgg.eval()\n",
    "        vgg_pretrained_features = vgg.features\n",
    "        #print(vgg_pretrained_features)\n",
    "        self.requires_grad = requires_grad\n",
    "        self.slice1 = torch.nn.Sequential()\n",
    "        self.slice2 = torch.nn.Sequential()\n",
    "        self.slice3 = torch.nn.Sequential()\n",
    "        self.slice4 = torch.nn.Sequential()\n",
    "        self.slice5 = torch.nn.Sequential()\n",
    "        for x in range(2): #(3):\n",
    "            self.slice1.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(2, 7): #(3, 7):\n",
    "            self.slice2.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(7, 12): #(7, 12):\n",
    "            self.slice3.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(12, 21): #(12, 21):\n",
    "            self.slice4.add_module(str(x), vgg_pretrained_features[x])\n",
    "        for x in range(21, 30):#(21, 30):\n",
    "            self.slice5.add_module(str(x), vgg_pretrained_features[x])\n",
    "        if not self.requires_grad:\n",
    "            for param in self.parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    " \n",
    "    def forward(self, X):\n",
    "        h=self.input(X)\n",
    "        h_relu1 = self.slice1(h)\n",
    "        h_relu2 = self.slice2(h_relu1)\n",
    "        h_relu3 = self.slice3(h_relu2)\n",
    "        h_relu4 = self.slice4(h_relu3)\n",
    "        h_relu5 = self.slice5(h_relu4)\n",
    "        out = [h_relu1, h_relu2, h_relu3, h_relu4, h_relu5]\n",
    "        return out\n",
    "\n",
    "class Perceptual_loss134(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptual_loss134, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        self.vgg = Vgg19_out().to(device)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.weights = [1.0/32, 1.0/4, 1.0/2, 1.0/1, 1.0/32]\n",
    "        # self.weights = [1.0/2.6, 1.0/16, 1.0/2, 1.0/1, 1.0]    \n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        # for i in range(len(x_vgg)-2):\n",
    "        #     loss += self.weights[i] * self.mse(x_vgg[i], y_vgg[i].detach())    \n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.L1(x_vgg[i], y_vgg[i].detach())        \n",
    "        return loss\n",
    "class Perceptual_loss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Perceptual_loss, self).__init__()\n",
    "        self.input=nn.Conv2d(in_channels=1 , out_channels=3, kernel_size=1)\n",
    "        self.vgg = Vgg19_out().to(device)\n",
    "        \n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.MS_SSIM_Loss = MS_SSIM_Loss2(data_range=1.0, size_average=True, channel=1)\n",
    "        #self.weights = [1.0/2.6, 1.0/16, 1.0/3.7, 1.0/5.6, 1.0]\n",
    "        self.weights = [1.0/15, 1.0/9, 1.0/4, 1.0/3, 1.0/1]    \n",
    "    def forward(self, x, y):\n",
    "        x_vgg, y_vgg = self.vgg(x), self.vgg(y)\n",
    "        loss = 0\n",
    "        # loss += self.weights[2] * self.L1(x_vgg[2], y_vgg[2].detach())\n",
    "        # loss += self.weights[4] * self.L1(x_vgg[4], y_vgg[4].detach())\n",
    "        # loss += self.weights[3] * self.mse(x_vgg[3], y_vgg[3].detach())\n",
    "        for i in range(len(x_vgg)):\n",
    "            loss += self.weights[i] * self.L1(x_vgg[i], y_vgg[i].detach()) \n",
    "        # for i in range(len(x_vgg)-1):\n",
    "        #     loss += self.weights[i] * self.MS_SSIM_Loss(x_vgg[i], y_vgg[i].detach())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "from torchmetrics.image.fid import FrechetInceptionDistance\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def calculate_fid_score(generator, testloader, device=device):\n",
    "    fid = FrechetInceptionDistance(feature=2048)#.to(device)\n",
    "    gen_features = []\n",
    "    real_features = []\n",
    "    generated_list = None  # Initialize generated_list variable\n",
    "    images_list = None  # Initialize images_list variable\n",
    "    generator.eval()\n",
    "    with torch.no_grad():\n",
    "        for idx, (x_img, y_img) in enumerate(testloader):\n",
    "            y_img1 = y_img.type(torch.cuda.FloatTensor).to(device)\n",
    "            x_img1 = x_img.type(torch.cuda.FloatTensor).to(device)\n",
    "            generated_images = generator(y_img1,x_img1)\n",
    "            generated_images = F.interpolate(generated_images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            generated_images_rgb = torch.cat([generated_images] * 3, dim=1)  # Convert grayscale to RGB\n",
    "            if generated_list is None:\n",
    "                generated_list = torch.cat((generated_images_rgb, generated_images_rgb), dim=0)\n",
    "            generated_list = torch.cat((generated_list, generated_images_rgb), dim=0)\n",
    "\n",
    "            images = y_img1\n",
    "            images = F.interpolate(images, size=(299, 299), mode='bilinear', align_corners=False)\n",
    "            images_rgb = torch.cat([images] * 3, dim=1)  # Convert grayscale to RGB\n",
    "            if images_list is None:\n",
    "                images_list = torch.cat((images_rgb, images_rgb), dim=0)\n",
    "            images_list = torch.cat((images_list, images_rgb), dim=0)\n",
    "\n",
    "    generated_list = generated_list.cpu().type(torch.uint8)  # Convert generated_list to torch.uint8\n",
    "    images_list = images_list.cpu().type(torch.uint8)  # Convert images_list to torch.uint8\n",
    "\n",
    "    fid.update(images_list, real=True)\n",
    "    fid.update(generated_list, real=False)\n",
    "    s= fid.compute()\n",
    "    return s    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "567c669a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import time\n",
    "class CycleGANsformer(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "        torch.cuda.set_device(1)\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "        self.discX = TDiscriminator().to(device)\n",
    "        self.device=device\n",
    "        self.genX2Y = TGenerator().to(device)\n",
    "        self.state_dict = {\n",
    "            'self.discX': self.discX.state_dict(),\n",
    "            'self.genX2Y': self.genX2Y.state_dict()\n",
    "        }\n",
    "        # checkpoint_path = './0429lite_checkpoint/0429lite_epoch90.pth'\n",
    "        # # Load the checkpoint\n",
    "        # checkpoint = torch.load(checkpoint_path)\n",
    "        # self.genX2Y.load_state_dict(checkpoint['self.genX2Y'])\n",
    "        # self.discX.load_state_dict(checkpoint['self.discX'])\n",
    "        \n",
    "        self.opt_discX = optim.AdamW(\n",
    "            self.discX.parameters(),\n",
    "            lr=5e-5,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "\n",
    "        self.opt_genX2Y = optim.AdamW(\n",
    "            self.genX2Y.parameters(),\n",
    "            lr=5e-5,\n",
    "            weight_decay=1e-5,\n",
    "        )\n",
    "        self.opt_genX2Y_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.opt_genX2Y, mode='min', factor=0.5, patience=10, verbose=True)#optim.lr_scheduler.StepLR(self.opt_genX2Y, step_size=20, gamma=0.5)\n",
    "        self.opt_discX_scheduler = optim.lr_scheduler.ReduceLROnPlateau(self.opt_discX, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "        self.L1 = nn.L1Loss()\n",
    "        self.mse = nn.MSELoss()\n",
    "        self.GradientDifferenceLoss = GDL()\n",
    "        self.HingeLoss = HingeLoss()\n",
    "        self.Perceptual_loss = Perceptual_loss134()\n",
    "        self.deblur_percept = Perceptual_loss()\n",
    "        self.batch_size=6\n",
    "    def fit(self, dataset, epochs=100):\n",
    "        loader = DataLoader(dataset, batch_size=self.batch_size, shuffle=True, num_workers=16, pin_memory=True)\n",
    "        # testloader=DataLoader(ImageDatasetLoader(mode=\"test\",transformers=transforms_ ), batch_size=self.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "        evlloader=DataLoader(ImageDatasetLoader(mode=\"evl\",transformers=transforms_ ), batch_size=self.batch_size, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "        \n",
    "        print(\"Start the training!\")\n",
    "        step = 0\n",
    "        for epoch in range(epochs):\n",
    "            for idx, (x_img, y_img) in enumerate(loader):\n",
    "                step+=1\n",
    "                y_img1=y_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                x_img1=x_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                self.genX2Y.train()\n",
    "                self.discX.train()\n",
    "#                 assert fake_x.size() == y_img1.size() , f\"fake_imgs.size(): {fake_x.size()} real_imgs.size(): {y_img1.size()}\"\n",
    "#######################################################################\n",
    "######################################################################                \n",
    "                self.opt_genX2Y.zero_grad()\n",
    "                fake_y1 = self.genX2Y(y_img1, x_img1)\n",
    "                D_Y_real1, segmap1 = self.discX(fake_y1, y_img1)\n",
    "                # D_Y_real1, segmap1 = self.discX(fake_y1[2], y_img1)\n",
    "            \n",
    "                generator_loss = self.HingeLoss(D_Y_real1,segmap1,real = False)\n",
    "                \n",
    "                fake_y2 = self.genX2Y(y_img1, x_img1)\n",
    "                perceptual_loss = self.Perceptual_loss(fake_y2,y_img1)\n",
    "                # perceptual_loss = self.Perceptual_loss(fake_y2[2],y_img1)\n",
    "                \n",
    "                cycle_X = self.genX2Y(y_img1, x_img1)\n",
    "                # GDL_loss = MS_SSIM_Loss(cycle_X,x_img1)*0.84 +  self.L1(cycle_X,x_img1)*0.16 #+ self.deblur_percept(cycle_X,x_img1)*0.7  #+ self.L1(cycle_X,x_img1)*0.05\n",
    "                deblur_percept_loss =  self.deblur_percept(cycle_X,x_img1)*1 + self.GradientDifferenceLoss(cycle_X,x_img1)*1 #+ self.L1(cycle_X,x_img1)*0.16# +  MS_SSIM_Loss(cycle_X,x_img1)*0.84 +  self.GradientDifferenceLoss(cycle_X,x_img1)*0.16 #self.deblur_percept(cycle_X,x_img1)\n",
    "                \n",
    "                G_loss = generator_loss*1 + perceptual_loss*0.15 + deblur_percept_loss *0.28   #+ GDL_loss *5\n",
    "                   \n",
    "                G_loss.backward(retain_graph=True)\n",
    "                self.opt_genX2Y.step()\n",
    "\n",
    "##################################################                \n",
    "                self.opt_discX.zero_grad()\n",
    "                D_X_real,seg_map2 = self.discX(x_img1,y_img1)\n",
    "                # with torch.no_grad():\n",
    "                #     fake_x = self.genX2Y(x_img1,y_img1)\n",
    "                #     fake_x = fake_x.detach()\n",
    "                #     fake_x.requires_grad_()\n",
    "                fake_x = self.genX2Y(y_img1, x_img1)\n",
    "                D_X_fake,seg_map3 = self.discX(fake_x,y_img1)\n",
    "                # D_X_fake,seg_map3 = self.discX(fake_x[2],y_img1)\n",
    "                D_real_loss = self.HingeLoss(D_X_real,seg_map2)\n",
    "                D_fake_loss = self.HingeLoss(D_X_fake,seg_map3,real = False)\n",
    "                D_X_loss = D_real_loss*1 + D_fake_loss*1\n",
    "                D_X_loss.backward(retain_graph=True)\n",
    "                self.opt_discX.step()\n",
    "##########################################################################\n",
    "##########################################################################\n",
    "            self.opt_genX2Y_scheduler.step(G_loss)\n",
    "            # self.opt_genX2Y_scheduler.step()\n",
    "            self.opt_discX_scheduler.step(D_X_loss)\n",
    "        \n",
    "\n",
    "        \n",
    "        \n",
    "            dir_checkpoint =\"./0429lite_checkpoint/\"\n",
    "            torch.save(self.state_dict, dir_checkpoint + f'0429lite_epoch{epoch}.pth')\n",
    "            print(f'[Epoch {epoch+1}/{epochs}]')\n",
    "            print(f'[G loss: {G_loss.item()} | generator_loss: {generator_loss.item()} |perceptual_loss: {perceptual_loss.item()} |deblur_percept_loss: {deblur_percept_loss.item()}]')\n",
    "            print(f'[D loss: { D_X_loss.item()} ]')\n",
    "#             ttest_real_A, ttest_real_B = next(iter(testloader))\n",
    "#             ans_ssim,ans_psnr_toB,ans_psnr_toA=sample_images(ttest_real_A, ttest_real_B,self.genX2Y,self.genY2X)\n",
    "            if (epoch) % 1 == 0:\n",
    "                self.genX2Y.eval()\n",
    "                with torch.no_grad():\n",
    "                    ans_ssim = 0\n",
    "                    ans_psnr_toB = 0\n",
    "                    ans_psnr_toA = 0\n",
    "                    for idx, (x_img, y_img) in enumerate(evlloader):\n",
    "                        y_img1=y_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                        x_img1=x_img.type(torch.cuda.FloatTensor).cuda(non_blocking=True)\n",
    "                        psnrA = PeakSignalNoiseRatio(data_range=torch.max(x_img1)-torch.min(x_img1)).cuda()\n",
    "                        psnrB = PeakSignalNoiseRatio(data_range=torch.max(y_img1)-torch.min(y_img1)).cuda()\n",
    "                        ssim = StructuralSimilarityIndexMeasure().cuda()\n",
    "                        fake_B = self.genX2Y(y_img1, x_img1)\n",
    "                        # fake_B = self.deblur(fake_B)\n",
    "\n",
    "                        ans_ssim +=ssim(fake_B,x_img1)\n",
    "                        ans_psnr_toB +=psnrB(fake_B,y_img1)\n",
    "                        ans_psnr_toA +=psnrA(fake_B,x_img1)\n",
    "\n",
    "                        if idx == len(evlloader)-1:\n",
    "\n",
    "                            nrows = x_img1.size(0)\n",
    "                            real_A = make_grid(x_img1, nrow=nrows, normalize=True)\n",
    "                            fake_B = make_grid(fake_B, nrow=nrows, normalize=True)\n",
    "                            real_B = make_grid(y_img1, nrow=nrows, normalize=True)\n",
    "\n",
    "                            image_grid = torch.cat((real_A, fake_B, real_B), 1).cpu().permute(1, 2, 0)\n",
    "\n",
    "                            plt.figure(figsize=(1.5*nrows, 1.5*3))\n",
    "                            plt.imshow(image_grid)\n",
    "                            plt.axis('off')\n",
    "                            plt.savefig(f'./0429lite_checkpoint/plot/{epoch}.png')\n",
    "                            plt.show()\n",
    "                            plt.close(image_grid)\n",
    "\n",
    "\n",
    "\n",
    "                    print(f\"Epoch {epoch} | ssim : {ans_ssim/len(evlloader)}\")\n",
    "                    print(f\"Epoch {epoch} | psnr_toB : {ans_psnr_toB/len(evlloader)}\")\n",
    "                    print(f\"Epoch {epoch} | psnr_toA : {ans_psnr_toA/len(evlloader)}\") \n",
    "\n",
    "                    \n",
    "                    cbct_count=0\n",
    "                    path=\"./0429/\"+str(epoch)+\"/\"\n",
    "                    if not os.path.exists(path):\n",
    "                        os.mkdir(path)\n",
    "                    cbct_data=[]\n",
    "                    for i in range(len(data)):\n",
    "                        ds=pydicom.dcmread(data[i][0],force=True)\n",
    "                        cbct_data.append(ds)\n",
    "                    cbctdata=cbct_data[24000:24704]\n",
    "                #     cbct_data=[]\n",
    "                #     for i in range(len(data)):\n",
    "                #         ds=pydicom.dcmread(data[i][0],force=True)\n",
    "                #         cbct_data.append(ds)\n",
    "\n",
    "                #     indices = [17000, 17010, 17020, 17030, 17040, 17060, 17070, 17080, 17090, 17100,\n",
    "                #            18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, \n",
    "                #            19000, 19050, 19150, 19250, 19350, 19450, 19550, 19650, 19750, 19850,\n",
    "                #            19950, 20050, 20150, 20250, 20350, 20450, 20550, 20650, 20750, 20850,\n",
    "                #            20950, 22030, 22130, 22230, 22330, 22430, 22530, 22630, 22730, 22830,\n",
    "                #            23070, 23170, 23270, 23370, 23470, 23570, 23670, 23770, 23870, 23970]\n",
    "\n",
    "                #     cbctdata = [cbct_data[i] for i in indices]\n",
    "\n",
    "                    start_time = time.time()\n",
    "                    for idx, (a_img, b_img) in enumerate(evlloader):\n",
    "\n",
    "                        model.eval()\n",
    "                        a=a_img.type(torch.cuda.FloatTensor)\n",
    "                        b=b_img.type(torch.cuda.FloatTensor)\n",
    "\n",
    "                        fake_B = model(b,a).detach()\n",
    "                        b = fake_B.cpu().numpy()\n",
    "                        c =b.copy()\n",
    "                        for s in range(len(a_img)):\n",
    "                            t=s+idx*batch_sizes\n",
    "                            write_dicom(cbctdata[t],c[s],path,t)\n",
    "                    end_time = time.time()\n",
    "                    execution_time = end_time - start_time\n",
    "                    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "93d96243",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device 1 NVIDIA A100-SXM4-80GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG19_Weights.IMAGENET1K_V1`. You can also use `weights=VGG19_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start the training!\n",
      "[Epoch 1/100]\n",
      "[G loss: 10.349475860595703 | generator_loss: 4.238254547119141 |perceptual_loss: 8.24566650390625 |deblur_percept_loss: 17.40846824645996]\n",
      "[D loss: 7.362360000610352 ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 216x324 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 | ssim : 0.5723037719726562\n",
      "Epoch 0 | psnr_toB : 23.947505950927734\n",
      "Epoch 0 | psnr_toA : 14.087447166442871\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m setb \u001b[38;5;241m=\u001b[39m ImageDatasetLoader(mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m\"\u001b[39m,transformers\u001b[38;5;241m=\u001b[39mdata_transform)\n\u001b[1;32m      7\u001b[0m trainset \u001b[38;5;241m=\u001b[39m ConcatDataset([seta, setb])\n\u001b[0;32m----> 8\u001b[0m \u001b[43mcg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainset\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [16]\u001b[0m, in \u001b[0;36mCycleGANsformer.fit\u001b[0;34m(self, dataset, epochs)\u001b[0m\n\u001b[1;32m    177\u001b[0m start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, (a_img, b_img) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(evlloader):\n\u001b[0;32m--> 180\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m    181\u001b[0m     a\u001b[38;5;241m=\u001b[39ma_img\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor)\n\u001b[1;32m    182\u001b[0m     b\u001b[38;5;241m=\u001b[39mb_img\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mFloatTensor)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "cg = CycleGANsformer()\n",
    "# cg.fit(ImageDatasetLoader(mode=\"train\",transformers=data_transform ))\n",
    "seta = ImageDatasetLoader(mode=\"train1\",transformers=transforms_)\n",
    "setb = ImageDatasetLoader(mode=\"train\",transformers=data_transform)\n",
    "trainset = ConcatDataset([seta, setb])\n",
    "cg.fit(trainset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a1086c6",
   "metadata": {},
   "source": [
    "wwwwww"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78c5cc1-e1e8-47e2-a0c5-4b4d9596b694",
   "metadata": {},
   "source": [
    "#### "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1f59faa-6a10-490e-8c3b-2f31b8290809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# checkpoint_path = './0429lite_checkpoint/0429lite_epoch99.pth'\n",
    "\n",
    "# # Load the checkpoint\n",
    "# checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "# torch.cuda.set_device(1)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "# model = TGenerator().to(device)\n",
    "# # Load the model state_dict from the checkpoint\n",
    "# model.load_state_dict(checkpoint['self.genX2Y'])\n",
    "\n",
    "# # Optionally, load other components such as optimizer state_dict or epoch number\n",
    "# # optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "# # epoch = checkpoint['epoch']\n",
    "\n",
    "# # Set the model in evaluation mode\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     fidloader=DataLoader(ImageDatasetLoader(mode=\"fid\",transformers=transforms_ ), batch_size=10, shuffle=False, num_workers=16)\n",
    "#     fretchet_dist=calculate_fid_score(model,fidloader) \n",
    "#     print( f\" fid : {fretchet_dist}\")  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015c3180-0c45-4f84-be38-0a8684996605",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "# def unnormalize(sample_dicom, x):\n",
    "#     ds = sample_dicom.copy()\n",
    "#     slope = ds.RescaleSlope\n",
    "#     intercept = ds.RescaleIntercept\n",
    "#     ymin = -200\n",
    "#     ymax = 1600\n",
    "#     pixel_min = ymin #/ slope + intercept\n",
    "#     pixel_max = ymax #/ slope + intercept\n",
    "# #     normalized_array = np.clip((x + 1) , 0, 2)\n",
    "# #     mean = np.mean(x)\n",
    "# #     std = np.std(x)\n",
    "# #     unnormalized_array = (x * std + mean) * slope + intercept\n",
    "#     scaled_array = (x)*5-600#(x*0.5+0.5)*100#-500\n",
    "#     unnormalized_array = (scaled_array - intercept) / slope \n",
    "#     return unnormalized_array\n",
    "def unnormalize(sample_dicom, x):\n",
    "    ds = sample_dicom.copy()\n",
    "    slope = ds.RescaleSlope\n",
    "    intercept = ds.RescaleIntercept\n",
    "    ymin = -200\n",
    "    ymax = 1600\n",
    "    pixel_min = ymin #/ slope + intercept\n",
    "    pixel_max = ymax #/ slope + intercept\n",
    "\n",
    "    scaled_array = (x)*6-700\n",
    "    unnormalized_array = (scaled_array - intercept) / slope \n",
    "    return unnormalized_array\n",
    "\n",
    "\n",
    "def write_dicom(sample_dicom, array, paths, tt):\n",
    "    ds = sample_dicom.copy()\n",
    "    ww = ds.WindowWidth\n",
    "    wc = ds.WindowCenter\n",
    "    array = np.transpose(array, (2, 1, 0))[:, :, 0]\n",
    "    array = unnormalize(sample_dicom, array)\n",
    "    array = np.clip(array, 0, 2 ** 16 - 1)\n",
    "    ds.PixelData = array.astype(np.uint16).tobytes()\n",
    "    return ds.save_as(paths + \"/\" + str(tt) + \".dcm\")\n",
    "checkpoint_path = './0429lite_checkpoint/0429lite_epoch99.pth'\n",
    "\n",
    "# Load the checkpoint\n",
    "checkpoint = torch.load(checkpoint_path)\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0,1,2,3'\n",
    "torch.cuda.set_device(1)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device\", torch.cuda.current_device(), torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "model = TGenerator().to(device)\n",
    "# Load the model state_dict from the checkpoint\n",
    "model.load_state_dict(checkpoint['self.genX2Y'])\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    cbct_count=0\n",
    "    batch_sizes = 2\n",
    "    testloader=DataLoader(ImageDatasetLoader(mode=\"test\",transformers=transforms_ ), batch_size=batch_sizes, shuffle=False, num_workers=16, pin_memory=True)\n",
    "    evlloader=DataLoader(ImageDatasetLoader(mode=\"evl\",transformers=transforms_ ), batch_size=batch_sizes, shuffle=False, num_workers=16, pin_memory=True)\n",
    "\n",
    "    path=\"./0429/\"+str(98)+\"/\"\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "    cbct_data=[]\n",
    "    for i in range(len(data)):\n",
    "        ds=pydicom.dcmread(data[i][0],force=True)\n",
    "        cbct_data.append(ds)\n",
    "    cbctdata=cbct_data[24000:24704]\n",
    "#     cbct_data=[]\n",
    "#     for i in range(len(data)):\n",
    "#         ds=pydicom.dcmread(data[i][0],force=True)\n",
    "#         cbct_data.append(ds)\n",
    "\n",
    "#     indices = [17000, 17010, 17020, 17030, 17040, 17060, 17070, 17080, 17090, 17100,\n",
    "#            18000, 18100, 18200, 18300, 18400, 18500, 18600, 18700, 18800, 18900, \n",
    "#            19000, 19050, 19150, 19250, 19350, 19450, 19550, 19650, 19750, 19850,\n",
    "#            19950, 20050, 20150, 20250, 20350, 20450, 20550, 20650, 20750, 20850,\n",
    "#            20950, 22030, 22130, 22230, 22330, 22430, 22530, 22630, 22730, 22830,\n",
    "#            23070, 23170, 23270, 23370, 23470, 23570, 23670, 23770, 23870, 23970]\n",
    "\n",
    "#     cbctdata = [cbct_data[i] for i in indices]\n",
    "    \n",
    "    start_time = time.time()\n",
    "    for idx, (a_img, b_img) in enumerate(evlloader):\n",
    "\n",
    "        model.eval()\n",
    "        a=a_img.type(torch.cuda.FloatTensor)\n",
    "        b=b_img.type(torch.cuda.FloatTensor)\n",
    "\n",
    "        fake_B = model(b,a).detach()\n",
    "        b = fake_B.cpu().numpy()\n",
    "        c =b.copy()\n",
    "        for s in range(len(a_img)):\n",
    "            t=s+idx*batch_sizes\n",
    "            write_dicom(cbctdata[t],c[s],path,t)\n",
    "    end_time = time.time()\n",
    "    execution_time = end_time - start_time\n",
    "    print(\"Execution time:\", execution_time, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10271062-9b99-43e9-8f27-6531201d5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "# Define the directory where the DICOM files are located\n",
    "directory = \"./0429/98/\"\n",
    "\n",
    "# Define the range of DICOM file names you want to load\n",
    "start_index = 0\n",
    "end_index = 103\n",
    "\n",
    "# Initialize a list to store the loaded DICOM datasets\n",
    "dicom_data = []\n",
    "\n",
    "# Loop through the range of file names and load each DICOM file\n",
    "for i in range(start_index, end_index + 1):\n",
    "    filename = os.path.join(directory, f\"{i}.dcm\")\n",
    "    if os.path.exists(filename):\n",
    "        ds = load_dcm(filename)\n",
    "        dicom_data.append(ds)\n",
    "# Replace these with your actual data\n",
    "scalar_value = 200\n",
    "scalar_list = [scalar_value] * len(dicom_data)\n",
    "x1 = np.array([x + scalar_value for x in dicom_data])\n",
    "x2 = np.array(data_load[24600:24704, 0] + scalar_value)\n",
    "scalar_value1 = 180\n",
    "scalar_list1 = [scalar_value1] * len(dicom_data)\n",
    "y = np.array(data_load[24600:24704, 1]+scalar_list1)\n",
    "\n",
    "# Reshape y to have a 2-dimensional shape (num_samples, 1)\n",
    "y = y.reshape(-1, 1)\n",
    "x1 = x1.reshape(-1, 1)\n",
    "x2 = x2.reshape(-1, 1)\n",
    "\n",
    "# Fit linear regression models for y with respect to x1 and x2, setting fit_intercept=False\n",
    "regressor_x1 = LinearRegression(fit_intercept=False)\n",
    "regressor_x1.fit(x1, y)\n",
    "\n",
    "regressor_x2 = LinearRegression(fit_intercept=False)\n",
    "regressor_x2.fit(x2, y)\n",
    "\n",
    "# Generate points for the regression lines\n",
    "x1_line = np.linspace(min(x1), max(x1), 100)\n",
    "x2_line = np.linspace(min(x2), max(x2), 100)\n",
    "\n",
    "y_line_x1 = regressor_x1.predict(x1_line.reshape(-1, 1))\n",
    "y_line_x2 = regressor_x2.predict(x2_line.reshape(-1, 1))\n",
    "# Plot the linear regression lines\n",
    "plt.plot(x1_line, y_line_x1, label='Linear Regression (sCT)', color='red')\n",
    "plt.plot(x2_line, y_line_x2, label='Linear Regression (CBCT)', color='blue')\n",
    "plt.plot(x1_line, x1_line, label='45-Degree Line', linestyle='dashed', color='green')\n",
    "\n",
    "plt.xlabel('sCT/CBCT values')\n",
    "plt.ylabel('planner CT values')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
